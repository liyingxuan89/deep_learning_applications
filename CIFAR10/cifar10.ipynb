{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyn1x/software/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#imports for model\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全局变量容器\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型的参数\n",
    "#tf.app.flags.DEFINE_integer(\"batch_size\", 128, \"\"\"number of images to process in one batch\"\"\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"./cifar_data\", \"\"\"path to the CIFAR10 data directory\"\"\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_fp16\", False, \"\"\"train the model use fp16\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 描述数据集的全局变量\n",
    "IMAGE_SIZE = cifar10_input.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN  = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型全局参数\n",
    "MOVEING_AVERAGE_DECAY = 0.9999\n",
    "NUM_EPOCHS_PER_DECAY = 350.0\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1\n",
    "INITIAL_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOWER_NAME = \"tower\"\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = re.sub(\"%s_[0-9]*/\" % TOWER_NAME, \"\", x.op.name)\n",
    "    tf.summary.histogram(tensor_name + \"/activation\", x)\n",
    "    tf.summary.scalar(tensor_name + \"/sparsity\", tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "        var = tf.get_variable(name, shape, initializer = initializer, dtype = dtype)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "    var = _variable_on_cpu(name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name = \"weight_loss\")\n",
    "        tf.add_to_collection(\"losses\", weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distorted_inputs():\n",
    "    if not FLAGS.data_dir:\n",
    "        raise ValueError(\"please supply a data directory.\")\n",
    "    data_dir = os.path.join(FLAGS.data_dir, \"cifar-10-batches-bin\")\n",
    "    images, labels = cifar10_input.distorted_inputs(data_dir = data_dir, batch_size = FLAGS.batch_size)\n",
    "    if FLAGES.use_fp16:\n",
    "        iamges = tf.cast(images, tf.float16)\n",
    "        labels = tf.cast(labels, tf.float16)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_data(eval_data):\n",
    "    if not FLAGS.data_dir:\n",
    "        raise ValueError(\"please supply a data_dir.\")\n",
    "    data_dir = os.path.join(FLAGS.data_dir, \"cifar-10-batches-bin\")\n",
    "    images, labels = cifar10_input.inputs(eval_data = eval_data, data_dir = data_dir, batch_size = FLAGS.batch_size)\n",
    "    if FLAGS.use_fg16:\n",
    "        images = tf.cast(images, tf.float16)\n",
    "        labels = tf.cast(labels, tf.float16)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "def inference(images):\n",
    "    # conv1\n",
    "    with tf.variable_scope(\"conv1\") as scope:\n",
    "        kernel = _variable_with_weight_decay(\"weights\", shape=[5,5,4,64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1,1,1,1], padding='SAME')\n",
    "        bias = _variable_on_cpu(\"biases\", [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "        \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME', name='pool1')\n",
    "    \n",
    "    #norm1\n",
    "    norm1 = tf.nn.lrn(pool1, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\"norm1\")\n",
    "    \n",
    "    #conv2 \n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        kernel = _variable_with_weight_decay(\"weights\", shape=[5,5,64,64], stddev=5e-2, wd=0.0)\n",
    "        conv = tf.nn.con2d(norm1, kernel, [1,1,1,1], padding='SAME')\n",
    "        bias = _variable_on_cpu(\"biases\", [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, bias)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "        \n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=.001/9.0, beta=.75, name='norm2')\n",
    "    \n",
    "    #pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], stride=[1,2,2,1], padding='SAME', name='pool2')\n",
    "    \n",
    "    #local3\n",
    "    with tf.variable_scope(\"local3\") as scope:\n",
    "        reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay(\"weights\", shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu(\"biases\", [384], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights)+biases, name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "    \n",
    "    #local4\n",
    "    with tf.variable_scope(\"local4\") as scope:\n",
    "        weights = _variable_with_weight_decay(\"weights\", shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu(\"biases\", [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights)+biases, name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "        \n",
    "    # fc3 linear layer\n",
    "    with tf.variable_scope(\"softmax_layer\") as scope:\n",
    "        weights = _variable_with_weight_decay(\"weights\", [192, NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "        biases = _variable_on_cpu(\"biases\", [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "        softmax_layer = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        _activation_summary(softmax_layer)\n",
    "    \n",
    "    return softmax_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss\n",
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.float64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels,\n",
    "                                                                  name=\"cross_entropy_per_example\")\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\"cross_entropy\")\n",
    "    tf.add_to_collection(\"losses\", cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection(\"losses\"), name=\"total_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\"avg\")\n",
    "    losses = tf.get_collection(\"losses\")\n",
    "    loss_averages_op = loss_averages.apply(lossed + [total_loss])\n",
    "    \n",
    "    for l in losses + [total_loss]:\n",
    "        tf.summary.scalar(l.op.name + \"(raw)\", l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "        \n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batch_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, decay_steps, LEARNING_RATE_DECAY_FACTOR, \n",
    "                                   staircase=True)\n",
    "    tf.summary.scalar(\"learning_rate\", lr)\n",
    "    \n",
    "    loss_average_op = _add_loss_summaries(total_loss)\n",
    "    \n",
    "    with tf.control_dependencies([loss_average_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "        \n",
    "    for grad, var, in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name+\"/gradients\", grad)\n",
    "    \n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVEING_AVERAGE_DECAY, global_step)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([apply_gradient_op, variable_averages_op]):\n",
    "        train_op = tf.no_op(name=\"train\")\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "    dest_directory = FLAGS.data_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "            float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
