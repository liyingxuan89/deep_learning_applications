{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 34528932.135845914)\n",
      "(1, 32233559.504211232)\n",
      "(2, 33637379.32163124)\n",
      "(3, 32729881.233941883)\n",
      "(4, 26543979.304093506)\n",
      "(5, 17349589.611975364)\n",
      "(6, 9544432.137678977)\n",
      "(7, 4954848.776044527)\n",
      "(8, 2728856.241914607)\n",
      "(9, 1709070.0824030486)\n",
      "(10, 1213497.9399404726)\n",
      "(11, 940729.4771918561)\n",
      "(12, 767401.122286196)\n",
      "(13, 643744.5150915214)\n",
      "(14, 548679.3419562577)\n",
      "(15, 472433.7791253474)\n",
      "(16, 409773.0596568176)\n",
      "(17, 357342.43863683194)\n",
      "(18, 313073.36784072954)\n",
      "(19, 275472.0360758031)\n",
      "(20, 243267.23757001435)\n",
      "(21, 215526.6762221221)\n",
      "(22, 191538.43857844072)\n",
      "(23, 170716.87442380248)\n",
      "(24, 152554.29188618512)\n",
      "(25, 136658.0918551303)\n",
      "(26, 122692.71781732657)\n",
      "(27, 110388.20657360664)\n",
      "(28, 99530.46966605597)\n",
      "(29, 89910.03271076197)\n",
      "(30, 81379.6314915779)\n",
      "(31, 73790.68902420388)\n",
      "(32, 67015.2552729126)\n",
      "(33, 60952.709488922104)\n",
      "(34, 55525.6496128752)\n",
      "(35, 50651.25484846148)\n",
      "(36, 46262.99761551886)\n",
      "(37, 42306.0591430617)\n",
      "(38, 38732.16030688332)\n",
      "(39, 35500.81493079493)\n",
      "(40, 32572.262539095434)\n",
      "(41, 29912.732488925823)\n",
      "(42, 27497.47844277572)\n",
      "(43, 25301.067002511045)\n",
      "(44, 23300.925417803126)\n",
      "(45, 21475.193766311797)\n",
      "(46, 19808.41498873807)\n",
      "(47, 18286.993162592982)\n",
      "(48, 16896.67671030807)\n",
      "(49, 15622.770898833576)\n",
      "(50, 14454.490158737975)\n",
      "(51, 13383.100195016057)\n",
      "(52, 12400.086722105412)\n",
      "(53, 11496.700799755978)\n",
      "(54, 10665.448011209286)\n",
      "(55, 9900.377695733125)\n",
      "(56, 9196.09036638642)\n",
      "(57, 8547.206562830963)\n",
      "(58, 7948.2439130540515)\n",
      "(59, 7395.427500682362)\n",
      "(60, 6884.47010912866)\n",
      "(61, 6412.042591398559)\n",
      "(62, 5974.836748666761)\n",
      "(63, 5570.120114161693)\n",
      "(64, 5195.438310740232)\n",
      "(65, 4848.0535847629035)\n",
      "(66, 4526.223839382791)\n",
      "(67, 4227.351175875704)\n",
      "(68, 3949.9470191064966)\n",
      "(69, 3692.2938617719956)\n",
      "(70, 3452.8451911561956)\n",
      "(71, 3230.185783796299)\n",
      "(72, 3023.2619486349613)\n",
      "(73, 2830.813489234802)\n",
      "(74, 2651.63485043646)\n",
      "(75, 2484.7444855015183)\n",
      "(76, 2329.410556659009)\n",
      "(77, 2184.5703007190214)\n",
      "(78, 2049.431031635794)\n",
      "(79, 1923.3804254826753)\n",
      "(80, 1805.7037850873892)\n",
      "(81, 1695.7817914273112)\n",
      "(82, 1593.0790446730812)\n",
      "(83, 1497.059807664782)\n",
      "(84, 1407.3052217250308)\n",
      "(85, 1323.3407742972322)\n",
      "(86, 1244.8237177936658)\n",
      "(87, 1171.399677205731)\n",
      "(88, 1102.6577527677964)\n",
      "(89, 1038.24742020998)\n",
      "(90, 977.8994704537077)\n",
      "(91, 921.3199092255481)\n",
      "(92, 868.272719543737)\n",
      "(93, 818.5561137493127)\n",
      "(94, 771.8887974307459)\n",
      "(95, 728.0807761349463)\n",
      "(96, 686.9344271297402)\n",
      "(97, 648.3020430182528)\n",
      "(98, 611.9967324185229)\n",
      "(99, 577.8612045216885)\n",
      "(100, 545.7698285334493)\n",
      "(101, 515.5839221744728)\n",
      "(102, 487.1979689234106)\n",
      "(103, 460.47465501138146)\n",
      "(104, 435.17786713349193)\n",
      "(105, 411.37144374952413)\n",
      "(106, 388.96650761901026)\n",
      "(107, 367.8704241331723)\n",
      "(108, 347.98902382305477)\n",
      "(109, 329.2565969773575)\n",
      "(110, 311.60602278796)\n",
      "(111, 294.9597677100848)\n",
      "(112, 279.26220244852107)\n",
      "(113, 264.45411628677005)\n",
      "(114, 250.48284322588955)\n",
      "(115, 237.29873177137827)\n",
      "(116, 224.8530850953091)\n",
      "(117, 213.10049538721387)\n",
      "(118, 202.00177478490036)\n",
      "(119, 191.5181864278971)\n",
      "(120, 181.6098654087471)\n",
      "(121, 172.24590691455896)\n",
      "(122, 163.3931327441245)\n",
      "(123, 155.0238485649666)\n",
      "(124, 147.107349198833)\n",
      "(125, 139.61819001511276)\n",
      "(126, 132.5308443268966)\n",
      "(127, 125.82656808143447)\n",
      "(128, 119.47980540110319)\n",
      "(129, 113.47060256790587)\n",
      "(130, 107.77982244242256)\n",
      "(131, 102.39127788746984)\n",
      "(132, 97.28591675421765)\n",
      "(133, 92.44863385295406)\n",
      "(134, 87.86660324079236)\n",
      "(135, 83.52162135077228)\n",
      "(136, 79.40255255285402)\n",
      "(137, 75.49766492647424)\n",
      "(138, 71.79358169270213)\n",
      "(139, 68.28155718023918)\n",
      "(140, 64.94910045134048)\n",
      "(141, 61.78751697782213)\n",
      "(142, 58.78774850676885)\n",
      "(143, 55.94029640417778)\n",
      "(144, 53.23672602153722)\n",
      "(145, 50.6704769477288)\n",
      "(146, 48.23352160703975)\n",
      "(147, 45.91883055222911)\n",
      "(148, 43.72056197303017)\n",
      "(149, 41.63182054816433)\n",
      "(150, 39.647847877446665)\n",
      "(151, 37.76210712811199)\n",
      "(152, 35.97017967950596)\n",
      "(153, 34.266517805295535)\n",
      "(154, 32.64707352433523)\n",
      "(155, 31.107536036330778)\n",
      "(156, 29.643178343769875)\n",
      "(157, 28.251034743302547)\n",
      "(158, 26.92647326432968)\n",
      "(159, 25.66710924602441)\n",
      "(160, 24.468207226953094)\n",
      "(161, 23.327735767731617)\n",
      "(162, 22.242383233554058)\n",
      "(163, 21.20931306422019)\n",
      "(164, 20.226087209309895)\n",
      "(165, 19.29016622113229)\n",
      "(166, 18.39898372295693)\n",
      "(167, 17.55047910418159)\n",
      "(168, 16.742407488096735)\n",
      "(169, 15.972869215571972)\n",
      "(170, 15.239811855979285)\n",
      "(171, 14.541701170638934)\n",
      "(172, 13.876608333952952)\n",
      "(173, 13.242932179317336)\n",
      "(174, 12.639316607700575)\n",
      "(175, 12.064056653663792)\n",
      "(176, 11.515869848065792)\n",
      "(177, 10.993282153175631)\n",
      "(178, 10.495178143343896)\n",
      "(179, 10.020474564686745)\n",
      "(180, 9.567755361118083)\n",
      "(181, 9.136128742001993)\n",
      "(182, 8.724551074364829)\n",
      "(183, 8.33208261253165)\n",
      "(184, 7.9574833776704565)\n",
      "(185, 7.600187470026118)\n",
      "(186, 7.259425724055256)\n",
      "(187, 6.934333429397113)\n",
      "(188, 6.62424886242877)\n",
      "(189, 6.328322989238018)\n",
      "(190, 6.046005460764356)\n",
      "(191, 5.776577906328777)\n",
      "(192, 5.519457959628371)\n",
      "(193, 5.2740579376200145)\n",
      "(194, 5.03984174817308)\n",
      "(195, 4.816250851316105)\n",
      "(196, 4.6028181358891604)\n",
      "(197, 4.3991106039040675)\n",
      "(198, 4.204591454307292)\n",
      "(199, 4.01888165480813)\n",
      "(200, 3.8415651724506725)\n",
      "(201, 3.6722485327757672)\n",
      "(202, 3.510533264538763)\n",
      "(203, 3.3561064616425096)\n",
      "(204, 3.208638327270622)\n",
      "(205, 3.06779986236394)\n",
      "(206, 2.9332510666915335)\n",
      "(207, 2.8047170256087433)\n",
      "(208, 2.6819592827987924)\n",
      "(209, 2.564670111952859)\n",
      "(210, 2.4525998253089423)\n",
      "(211, 2.34553309565992)\n",
      "(212, 2.243243024697567)\n",
      "(213, 2.1454781944250696)\n",
      "(214, 2.0520697599267583)\n",
      "(215, 1.9628091527352804)\n",
      "(216, 1.8774896133895915)\n",
      "(217, 1.7959462382294098)\n",
      "(218, 1.7180232478128974)\n",
      "(219, 1.6435402105126888)\n",
      "(220, 1.5723352744730166)\n",
      "(221, 1.5042688226440855)\n",
      "(222, 1.439211829725142)\n",
      "(223, 1.3770211425893204)\n",
      "(224, 1.3175486436999202)\n",
      "(225, 1.2606910637093105)\n",
      "(226, 1.2063318589847865)\n",
      "(227, 1.154350831306922)\n",
      "(228, 1.104641526863896)\n",
      "(229, 1.0571146861405214)\n",
      "(230, 1.0116689462502526)\n",
      "(231, 0.968197651240796)\n",
      "(232, 0.9266283574505028)\n",
      "(233, 0.8868787762905253)\n",
      "(234, 0.8488495183854049)\n",
      "(235, 0.8124728188905331)\n",
      "(236, 0.7776862211490443)\n",
      "(237, 0.7444107320469173)\n",
      "(238, 0.7125772884886585)\n",
      "(239, 0.6821235649405748)\n",
      "(240, 0.6529949523895885)\n",
      "(241, 0.6251209274098296)\n",
      "(242, 0.5984535360928349)\n",
      "(243, 0.5729441983914599)\n",
      "(244, 0.548535253871091)\n",
      "(245, 0.5251816245317733)\n",
      "(246, 0.5028332595110157)\n",
      "(247, 0.4814524774952662)\n",
      "(248, 0.46099068960694567)\n",
      "(249, 0.4414087832365947)\n",
      "(250, 0.42267082539106304)\n",
      "(251, 0.404738287209231)\n",
      "(252, 0.38757432755734766)\n",
      "(253, 0.3711491642218551)\n",
      "(254, 0.35542887207480867)\n",
      "(255, 0.34038149605081436)\n",
      "(256, 0.32597788360830776)\n",
      "(257, 0.31219320657938104)\n",
      "(258, 0.29899968448847386)\n",
      "(259, 0.28636782399800864)\n",
      "(260, 0.2742766510349896)\n",
      "(261, 0.26270247071621605)\n",
      "(262, 0.2516213083203106)\n",
      "(263, 0.24101224890505063)\n",
      "(264, 0.23085782388211978)\n",
      "(265, 0.2211373076817747)\n",
      "(266, 0.21182775394501002)\n",
      "(267, 0.20291398026374333)\n",
      "(268, 0.19438061601808787)\n",
      "(269, 0.18620923364978864)\n",
      "(270, 0.17838500532864524)\n",
      "(271, 0.17089440455838764)\n",
      "(272, 0.16372134346226958)\n",
      "(273, 0.15685119861242972)\n",
      "(274, 0.15027329256220018)\n",
      "(275, 0.14397326847407915)\n",
      "(276, 0.13794045855333234)\n",
      "(277, 0.13216239681700676)\n",
      "(278, 0.12662930559097085)\n",
      "(279, 0.12133009334204466)\n",
      "(280, 0.11625465031013775)\n",
      "(281, 0.11139348104342331)\n",
      "(282, 0.10673817994762057)\n",
      "(283, 0.1022788449079518)\n",
      "(284, 0.09800813499047517)\n",
      "(285, 0.09391660531579611)\n",
      "(286, 0.08999793673553966)\n",
      "(287, 0.0862440756518468)\n",
      "(288, 0.08264813804119019)\n",
      "(289, 0.07920362303539985)\n",
      "(290, 0.07590389228883214)\n",
      "(291, 0.07274268809342646)\n",
      "(292, 0.06971452674531786)\n",
      "(293, 0.06681403377639811)\n",
      "(294, 0.06403463667052525)\n",
      "(295, 0.06137170270919738)\n",
      "(296, 0.05882070511671963)\n",
      "(297, 0.0563769460700454)\n",
      "(298, 0.05403519502401248)\n",
      "(299, 0.05179138425925364)\n",
      "(300, 0.04964172806648111)\n",
      "(301, 0.04758192098314823)\n",
      "(302, 0.045608468682065764)\n",
      "(303, 0.04371743419240071)\n",
      "(304, 0.041905381684769386)\n",
      "(305, 0.04016890905770254)\n",
      "(306, 0.03850523979497817)\n",
      "(307, 0.03691095985038562)\n",
      "(308, 0.03538312220018173)\n",
      "(309, 0.033919181211873234)\n",
      "(310, 0.032516138226646285)\n",
      "(311, 0.031171714070091783)\n",
      "(312, 0.029883045588122853)\n",
      "(313, 0.028648169401398492)\n",
      "(314, 0.02746466916646752)\n",
      "(315, 0.026330419532088294)\n",
      "(316, 0.02524335777992926)\n",
      "(317, 0.02420143989062203)\n",
      "(318, 0.023203084751079607)\n",
      "(319, 0.02224605359305821)\n",
      "(320, 0.021328668736377406)\n",
      "(321, 0.0204495185932804)\n",
      "(322, 0.01960685686179326)\n",
      "(323, 0.018799123176922262)\n",
      "(324, 0.018024830098873842)\n",
      "(325, 0.01728265541736792)\n",
      "(326, 0.01657131400429343)\n",
      "(327, 0.015889416830442246)\n",
      "(328, 0.015235720480395088)\n",
      "(329, 0.014609130618589351)\n",
      "(330, 0.014008468572558493)\n",
      "(331, 0.013432674915298903)\n",
      "(332, 0.012880711610757836)\n",
      "(333, 0.012351595046290777)\n",
      "(334, 0.011844411432039887)\n",
      "(335, 0.011358064021172377)\n",
      "(336, 0.010891832778045254)\n",
      "(337, 0.010444893825098538)\n",
      "(338, 0.010016345655360042)\n",
      "(339, 0.009605490554604678)\n",
      "(340, 0.009211602637004008)\n",
      "(341, 0.008833988546970852)\n",
      "(342, 0.008471938435412326)\n",
      "(343, 0.008124816624775752)\n",
      "(344, 0.007792026279548348)\n",
      "(345, 0.007472917864365804)\n",
      "(346, 0.007166968905970456)\n",
      "(347, 0.006873601770560981)\n",
      "(348, 0.00659233343706475)\n",
      "(349, 0.006322613160460129)\n",
      "(350, 0.006064014029718556)\n",
      "(351, 0.005816025143751598)\n",
      "(352, 0.005578239221236338)\n",
      "(353, 0.005350218850312886)\n",
      "(354, 0.005131614709306696)\n",
      "(355, 0.0049219733115847784)\n",
      "(356, 0.004720934414777226)\n",
      "(357, 0.00452814763988667)\n",
      "(358, 0.004343281076274045)\n",
      "(359, 0.004166018293161913)\n",
      "(360, 0.003996012009959683)\n",
      "(361, 0.003832967952521803)\n",
      "(362, 0.003676608044145486)\n",
      "(363, 0.003526673419497935)\n",
      "(364, 0.0033828896858531868)\n",
      "(365, 0.003244984934953622)\n",
      "(366, 0.0031127296450955684)\n",
      "(367, 0.0029859017537841857)\n",
      "(368, 0.002864255261203217)\n",
      "(369, 0.0027475974346229783)\n",
      "(370, 0.002635707674844366)\n",
      "(371, 0.002528416112027219)\n",
      "(372, 0.002425491332908299)\n",
      "(373, 0.0023267813481513073)\n",
      "(374, 0.0022321137853158635)\n",
      "(375, 0.0021413169280195448)\n",
      "(376, 0.002054222238092886)\n",
      "(377, 0.001970685935436073)\n",
      "(378, 0.001890565995288216)\n",
      "(379, 0.0018137173272181895)\n",
      "(380, 0.001740007454699713)\n",
      "(381, 0.001669302407739969)\n",
      "(382, 0.0016014872453271594)\n",
      "(383, 0.001536449030499586)\n",
      "(384, 0.0014740521765297856)\n",
      "(385, 0.00141420619293619)\n",
      "(386, 0.0013567940155640099)\n",
      "(387, 0.0013017264030446862)\n",
      "(388, 0.0012489040274869063)\n",
      "(389, 0.0011982307635728284)\n",
      "(390, 0.0011496326149770995)\n",
      "(391, 0.0011030041664942378)\n",
      "(392, 0.0010582726904938894)\n",
      "(393, 0.0010153655979350915)\n",
      "(394, 0.0009742044394972767)\n",
      "(395, 0.0009347287673493506)\n",
      "(396, 0.0008968506790548144)\n",
      "(397, 0.0008605137636127763)\n",
      "(398, 0.000825655793939694)\n",
      "(399, 0.000792214987603066)\n",
      "(400, 0.0007601312584663255)\n",
      "(401, 0.000729352319060465)\n",
      "(402, 0.0006998247176553418)\n",
      "(403, 0.0006714996349702461)\n",
      "(404, 0.0006443235286025244)\n",
      "(405, 0.0006182513588368396)\n",
      "(406, 0.0005932411906290221)\n",
      "(407, 0.0005692482881692971)\n",
      "(408, 0.0005462249011571878)\n",
      "(409, 0.0005241363998478582)\n",
      "(410, 0.0005029437638136912)\n",
      "(411, 0.0004826125190508884)\n",
      "(412, 0.00046310510570773597)\n",
      "(413, 0.00044438883194170383)\n",
      "(414, 0.0004264321831758202)\n",
      "(415, 0.0004092035532422132)\n",
      "(416, 0.0003926741696929888)\n",
      "(417, 0.0003768149074174228)\n",
      "(418, 0.00036159711609751714)\n",
      "(419, 0.00034699906087920485)\n",
      "(420, 0.00033299040164115227)\n",
      "(421, 0.0003195489180179112)\n",
      "(422, 0.00030665122645091836)\n",
      "(423, 0.0002942758982280902)\n",
      "(424, 0.00028240292114740153)\n",
      "(425, 0.00027100955580787754)\n",
      "(426, 0.00026007692986288074)\n",
      "(427, 0.00024958830802679366)\n",
      "(428, 0.00023952318691487312)\n",
      "(429, 0.00022986526763253928)\n",
      "(430, 0.0002205979716203619)\n",
      "(431, 0.00021170629365153875)\n",
      "(432, 0.0002031731694969722)\n",
      "(433, 0.00019498532680145737)\n",
      "(434, 0.00018712800507691277)\n",
      "(435, 0.00017958832171793913)\n",
      "(436, 0.00017235426635553105)\n",
      "(437, 0.00016541194743475632)\n",
      "(438, 0.00015875030477000246)\n",
      "(439, 0.0001523574308441385)\n",
      "(440, 0.0001462225437514468)\n",
      "(441, 0.000140335961420976)\n",
      "(442, 0.00013468695352849536)\n",
      "(443, 0.0001292663113093197)\n",
      "(444, 0.00012406397830583358)\n",
      "(445, 0.00011907165154252199)\n",
      "(446, 0.00011428059481536485)\n",
      "(447, 0.00010968289158390702)\n",
      "(448, 0.00010527078298020129)\n",
      "(449, 0.00010103699259279118)\n",
      "(450, 9.697355316593657e-05)\n",
      "(451, 9.307393830261477e-05)\n",
      "(452, 8.933165356558116e-05)\n",
      "(453, 8.574014331001058e-05)\n",
      "(454, 8.229367588168672e-05)\n",
      "(455, 7.898651755422885e-05)\n",
      "(456, 7.581195300214548e-05)\n",
      "(457, 7.276530542067969e-05)\n",
      "(458, 6.98416232766818e-05)\n",
      "(459, 6.70358774278211e-05)\n",
      "(460, 6.434312302685622e-05)\n",
      "(461, 6.175855720738864e-05)\n",
      "(462, 5.927801365536671e-05)\n",
      "(463, 5.68974763011456e-05)\n",
      "(464, 5.461276194481998e-05)\n",
      "(465, 5.241993508769827e-05)\n",
      "(466, 5.031540062340124e-05)\n",
      "(467, 4.82958649956183e-05)\n",
      "(468, 4.6357327977426214e-05)\n",
      "(469, 4.449672746967777e-05)\n",
      "(470, 4.271112142302793e-05)\n",
      "(471, 4.0997320406806766e-05)\n",
      "(472, 3.9352473032760035e-05)\n",
      "(473, 3.777370367821549e-05)\n",
      "(474, 3.625841070818347e-05)\n",
      "(475, 3.480399193602569e-05)\n",
      "(476, 3.340814216755935e-05)\n",
      "(477, 3.206838403473251e-05)\n",
      "(478, 3.0782440393280184e-05)\n",
      "(479, 2.954829842343536e-05)\n",
      "(480, 2.836358668344715e-05)\n",
      "(481, 2.7226584587809645e-05)\n",
      "(482, 2.613536427188112e-05)\n",
      "(483, 2.5087791989891033e-05)\n",
      "(484, 2.408230524461171e-05)\n",
      "(485, 2.3117214461791027e-05)\n",
      "(486, 2.2190920083428482e-05)\n",
      "(487, 2.1301811830057435e-05)\n",
      "(488, 2.0448400757587385e-05)\n",
      "(489, 1.962927562221412e-05)\n",
      "(490, 1.8842986279282325e-05)\n",
      "(491, 1.8088373187941595e-05)\n",
      "(492, 1.73639332452033e-05)\n",
      "(493, 1.666862559155921e-05)\n",
      "(494, 1.600119113562516e-05)\n",
      "(495, 1.53605045615336e-05)\n",
      "(496, 1.474555532902638e-05)\n",
      "(497, 1.4155225933355279e-05)\n",
      "(498, 1.3588574070862878e-05)\n",
      "(499, 1.3044663683563533e-05)\n"
     ]
    }
   ],
   "source": [
    "#using numpy\n",
    "import numpy as np\n",
    "# N batch_size; D_in input dimention\n",
    "# H hidden dimention; D_out output dimention\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "#randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # backprop to compute gradient of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing.\n",
    "\n",
    "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we need to manually implement the forward and backward passes through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "(0, 31173308.0)\n",
      "(1, 30147892.0)\n",
      "(2, 33895816.0)\n",
      "(3, 36641920.0)\n",
      "(4, 33344694.0)\n",
      "(5, 23637898.0)\n",
      "(6, 13210258.0)\n",
      "(7, 6450733.0)\n",
      "(8, 3230327.75)\n",
      "(9, 1859098.25)\n",
      "(10, 1257263.75)\n",
      "(11, 953752.3125)\n",
      "(12, 771352.5625)\n",
      "(13, 644351.75)\n",
      "(14, 547311.125)\n",
      "(15, 469421.375)\n",
      "(16, 405337.84375)\n",
      "(17, 351865.5625)\n",
      "(18, 306817.46875)\n",
      "(19, 268627.5)\n",
      "(20, 236045.046875)\n",
      "(21, 208101.25)\n",
      "(22, 184039.0)\n",
      "(23, 163241.125)\n",
      "(24, 145184.921875)\n",
      "(25, 129455.8125)\n",
      "(26, 115717.9921875)\n",
      "(27, 103673.90625)\n",
      "(28, 93088.703125)\n",
      "(29, 83751.328125)\n",
      "(30, 75489.9921875)\n",
      "(31, 68163.765625)\n",
      "(32, 61651.75)\n",
      "(33, 55847.328125)\n",
      "(34, 50664.68359375)\n",
      "(35, 46032.5625)\n",
      "(36, 41883.4921875)\n",
      "(37, 38157.43359375)\n",
      "(38, 34803.94140625)\n",
      "(39, 31783.60546875)\n",
      "(40, 29064.1171875)\n",
      "(41, 26612.8359375)\n",
      "(42, 24392.48828125)\n",
      "(43, 22378.384765625)\n",
      "(44, 20549.484375)\n",
      "(45, 18885.083984375)\n",
      "(46, 17369.515625)\n",
      "(47, 15987.884765625)\n",
      "(48, 14726.8359375)\n",
      "(49, 13575.1591796875)\n",
      "(50, 12522.078125)\n",
      "(51, 11557.5751953125)\n",
      "(52, 10675.2490234375)\n",
      "(53, 9868.330078125)\n",
      "(54, 9127.6884765625)\n",
      "(55, 8447.119140625)\n",
      "(56, 7821.27099609375)\n",
      "(57, 7245.396484375)\n",
      "(58, 6715.677734375)\n",
      "(59, 6227.53125)\n",
      "(60, 5777.67919921875)\n",
      "(61, 5362.42578125)\n",
      "(62, 4979.12255859375)\n",
      "(63, 4625.09326171875)\n",
      "(64, 4297.79296875)\n",
      "(65, 3995.239013671875)\n",
      "(66, 3715.3203125)\n",
      "(67, 3456.209716796875)\n",
      "(68, 3216.24267578125)\n",
      "(69, 2993.93798828125)\n",
      "(70, 2787.82275390625)\n",
      "(71, 2596.84814453125)\n",
      "(72, 2419.7373046875)\n",
      "(73, 2255.357666015625)\n",
      "(74, 2102.75341796875)\n",
      "(75, 1961.0185546875)\n",
      "(76, 1829.30615234375)\n",
      "(77, 1706.8992919921875)\n",
      "(78, 1593.1995849609375)\n",
      "(79, 1487.38623046875)\n",
      "(80, 1388.95556640625)\n",
      "(81, 1297.3497314453125)\n",
      "(82, 1212.087646484375)\n",
      "(83, 1132.722412109375)\n",
      "(84, 1058.83447265625)\n",
      "(85, 989.9796142578125)\n",
      "(86, 925.7925415039062)\n",
      "(87, 865.964111328125)\n",
      "(88, 810.178466796875)\n",
      "(89, 758.1300048828125)\n",
      "(90, 709.58203125)\n",
      "(91, 664.2838134765625)\n",
      "(92, 621.9957885742188)\n",
      "(93, 582.5214233398438)\n",
      "(94, 545.677978515625)\n",
      "(95, 511.26165771484375)\n",
      "(96, 479.1181640625)\n",
      "(97, 449.0804138183594)\n",
      "(98, 420.9947509765625)\n",
      "(99, 394.7386474609375)\n",
      "(100, 370.16876220703125)\n",
      "(101, 347.1896057128906)\n",
      "(102, 325.69122314453125)\n",
      "(103, 305.5935974121094)\n",
      "(104, 286.7691650390625)\n",
      "(105, 269.14288330078125)\n",
      "(106, 252.63644409179688)\n",
      "(107, 237.17645263671875)\n",
      "(108, 222.69517517089844)\n",
      "(109, 209.1940155029297)\n",
      "(110, 196.5408935546875)\n",
      "(111, 184.67477416992188)\n",
      "(112, 173.5540771484375)\n",
      "(113, 163.12937927246094)\n",
      "(114, 153.3475341796875)\n",
      "(115, 144.17135620117188)\n",
      "(116, 135.5634765625)\n",
      "(117, 127.48675537109375)\n",
      "(118, 119.90570068359375)\n",
      "(119, 112.7884292602539)\n",
      "(120, 106.10798645019531)\n",
      "(121, 99.83514404296875)\n",
      "(122, 93.94493103027344)\n",
      "(123, 88.41427612304688)\n",
      "(124, 83.22428131103516)\n",
      "(125, 78.34219360351562)\n",
      "(126, 73.75621795654297)\n",
      "(127, 69.44537353515625)\n",
      "(128, 65.3963851928711)\n",
      "(129, 61.58979797363281)\n",
      "(130, 58.01118850708008)\n",
      "(131, 54.647560119628906)\n",
      "(132, 51.483009338378906)\n",
      "(133, 48.506439208984375)\n",
      "(134, 45.708831787109375)\n",
      "(135, 43.077667236328125)\n",
      "(136, 40.600547790527344)\n",
      "(137, 38.26935958862305)\n",
      "(138, 36.07643508911133)\n",
      "(139, 34.012725830078125)\n",
      "(140, 32.0704345703125)\n",
      "(141, 30.242103576660156)\n",
      "(142, 28.520647048950195)\n",
      "(143, 26.89983367919922)\n",
      "(144, 25.373210906982422)\n",
      "(145, 23.93661880493164)\n",
      "(146, 22.58382797241211)\n",
      "(147, 21.3095703125)\n",
      "(148, 20.107620239257812)\n",
      "(149, 18.97628402709961)\n",
      "(150, 17.90977668762207)\n",
      "(151, 16.905010223388672)\n",
      "(152, 15.957956314086914)\n",
      "(153, 15.065481185913086)\n",
      "(154, 14.22414493560791)\n",
      "(155, 13.430944442749023)\n",
      "(156, 12.683300018310547)\n",
      "(157, 11.978711128234863)\n",
      "(158, 11.313423156738281)\n",
      "(159, 10.686171531677246)\n",
      "(160, 10.094724655151367)\n",
      "(161, 9.536927223205566)\n",
      "(162, 9.010744094848633)\n",
      "(163, 8.514362335205078)\n",
      "(164, 8.045440673828125)\n",
      "(165, 7.603466510772705)\n",
      "(166, 7.186136245727539)\n",
      "(167, 6.792564392089844)\n",
      "(168, 6.420926094055176)\n",
      "(169, 6.07017183303833)\n",
      "(170, 5.739053726196289)\n",
      "(171, 5.426116466522217)\n",
      "(172, 5.131193161010742)\n",
      "(173, 4.852287769317627)\n",
      "(174, 4.588833808898926)\n",
      "(175, 4.340236663818359)\n",
      "(176, 4.105340957641602)\n",
      "(177, 3.8835880756378174)\n",
      "(178, 3.673936605453491)\n",
      "(179, 3.4759409427642822)\n",
      "(180, 3.2891407012939453)\n",
      "(181, 3.1121840476989746)\n",
      "(182, 2.945178508758545)\n",
      "(183, 2.7870960235595703)\n",
      "(184, 2.637909173965454)\n",
      "(185, 2.4969499111175537)\n",
      "(186, 2.363497734069824)\n",
      "(187, 2.2374091148376465)\n",
      "(188, 2.118194818496704)\n",
      "(189, 2.0055809020996094)\n",
      "(190, 1.899120569229126)\n",
      "(191, 1.7982574701309204)\n",
      "(192, 1.7030237913131714)\n",
      "(193, 1.6129403114318848)\n",
      "(194, 1.5276566743850708)\n",
      "(195, 1.4469482898712158)\n",
      "(196, 1.3707276582717896)\n",
      "(197, 1.2984297275543213)\n",
      "(198, 1.230212926864624)\n",
      "(199, 1.165522813796997)\n",
      "(200, 1.1044743061065674)\n",
      "(201, 1.0465095043182373)\n",
      "(202, 0.9918090105056763)\n",
      "(203, 0.9400773644447327)\n",
      "(204, 0.8909026384353638)\n",
      "(205, 0.8444182872772217)\n",
      "(206, 0.800503134727478)\n",
      "(207, 0.7588432431221008)\n",
      "(208, 0.7193856835365295)\n",
      "(209, 0.6820806264877319)\n",
      "(210, 0.6467667818069458)\n",
      "(211, 0.6132977604866028)\n",
      "(212, 0.5815439820289612)\n",
      "(213, 0.5515039563179016)\n",
      "(214, 0.5230374336242676)\n",
      "(215, 0.49611663818359375)\n",
      "(216, 0.4706217646598816)\n",
      "(217, 0.44641411304473877)\n",
      "(218, 0.4234951436519623)\n",
      "(219, 0.401752233505249)\n",
      "(220, 0.38117462396621704)\n",
      "(221, 0.3616604804992676)\n",
      "(222, 0.34322357177734375)\n",
      "(223, 0.3256931006908417)\n",
      "(224, 0.3091205656528473)\n",
      "(225, 0.2932995557785034)\n",
      "(226, 0.2784082293510437)\n",
      "(227, 0.264242559671402)\n",
      "(228, 0.2508292496204376)\n",
      "(229, 0.23813137412071228)\n",
      "(230, 0.2260655164718628)\n",
      "(231, 0.21461409330368042)\n",
      "(232, 0.20377254486083984)\n",
      "(233, 0.1934872567653656)\n",
      "(234, 0.18373778462409973)\n",
      "(235, 0.17446818947792053)\n",
      "(236, 0.16570472717285156)\n",
      "(237, 0.1573571264743805)\n",
      "(238, 0.14947658777236938)\n",
      "(239, 0.1419840157032013)\n",
      "(240, 0.13488763570785522)\n",
      "(241, 0.12812809646129608)\n",
      "(242, 0.12171688675880432)\n",
      "(243, 0.11565379798412323)\n",
      "(244, 0.10987359285354614)\n",
      "(245, 0.10439588874578476)\n",
      "(246, 0.09918875247240067)\n",
      "(247, 0.09426465630531311)\n",
      "(248, 0.08958897739648819)\n",
      "(249, 0.08512754738330841)\n",
      "(250, 0.08092456310987473)\n",
      "(251, 0.07691612839698792)\n",
      "(252, 0.07310539484024048)\n",
      "(253, 0.0695057213306427)\n",
      "(254, 0.0660763680934906)\n",
      "(255, 0.06282113492488861)\n",
      "(256, 0.05971334129571915)\n",
      "(257, 0.05677076056599617)\n",
      "(258, 0.053983014076948166)\n",
      "(259, 0.05134424567222595)\n",
      "(260, 0.04882512986660004)\n",
      "(261, 0.04642801359295845)\n",
      "(262, 0.04416121542453766)\n",
      "(263, 0.04200625419616699)\n",
      "(264, 0.03994806855916977)\n",
      "(265, 0.03799169510602951)\n",
      "(266, 0.03615596145391464)\n",
      "(267, 0.03438963741064072)\n",
      "(268, 0.03272552788257599)\n",
      "(269, 0.031136687844991684)\n",
      "(270, 0.029635969549417496)\n",
      "(271, 0.028205841779708862)\n",
      "(272, 0.0268383901566267)\n",
      "(273, 0.02554319240152836)\n",
      "(274, 0.024309052154421806)\n",
      "(275, 0.02315068244934082)\n",
      "(276, 0.022031815722584724)\n",
      "(277, 0.020971599966287613)\n",
      "(278, 0.019971415400505066)\n",
      "(279, 0.01902000978589058)\n",
      "(280, 0.018101664260029793)\n",
      "(281, 0.01724468730390072)\n",
      "(282, 0.016422465443611145)\n",
      "(283, 0.015647778287529945)\n",
      "(284, 0.014906203374266624)\n",
      "(285, 0.014186865650117397)\n",
      "(286, 0.013528803363442421)\n",
      "(287, 0.01289177406579256)\n",
      "(288, 0.012284323573112488)\n",
      "(289, 0.011708365753293037)\n",
      "(290, 0.011162089183926582)\n",
      "(291, 0.010642205365002155)\n",
      "(292, 0.01014002226293087)\n",
      "(293, 0.009674811735749245)\n",
      "(294, 0.009224427863955498)\n",
      "(295, 0.008800867944955826)\n",
      "(296, 0.008391275070607662)\n",
      "(297, 0.00800914317369461)\n",
      "(298, 0.007640551775693893)\n",
      "(299, 0.007292686961591244)\n",
      "(300, 0.0069594606757164)\n",
      "(301, 0.006641217041760683)\n",
      "(302, 0.006340147461742163)\n",
      "(303, 0.006055572535842657)\n",
      "(304, 0.005781381390988827)\n",
      "(305, 0.005525063723325729)\n",
      "(306, 0.005276795942336321)\n",
      "(307, 0.005043643061071634)\n",
      "(308, 0.004817522130906582)\n",
      "(309, 0.0046037775464355946)\n",
      "(310, 0.0044024670496582985)\n",
      "(311, 0.0042084986343979836)\n",
      "(312, 0.004021011758595705)\n",
      "(313, 0.0038451477885246277)\n",
      "(314, 0.0036786848213523626)\n",
      "(315, 0.003520816331729293)\n",
      "(316, 0.003370031714439392)\n",
      "(317, 0.003223547711968422)\n",
      "(318, 0.0030875944066792727)\n",
      "(319, 0.002958365250378847)\n",
      "(320, 0.0028286552987992764)\n",
      "(321, 0.002709292573854327)\n",
      "(322, 0.0025972104631364346)\n",
      "(323, 0.002488969126716256)\n",
      "(324, 0.002386996988207102)\n",
      "(325, 0.0022883208002895117)\n",
      "(326, 0.002190501894801855)\n",
      "(327, 0.0021026157774031162)\n",
      "(328, 0.0020190675277262926)\n",
      "(329, 0.0019373195245862007)\n",
      "(330, 0.001858701929450035)\n",
      "(331, 0.0017835540929809213)\n",
      "(332, 0.0017126455204561353)\n",
      "(333, 0.0016434581484645605)\n",
      "(334, 0.0015782892005518079)\n",
      "(335, 0.0015199771150946617)\n",
      "(336, 0.0014592739753425121)\n",
      "(337, 0.0014038848457857966)\n",
      "(338, 0.0013497117906808853)\n",
      "(339, 0.00129763747099787)\n",
      "(340, 0.0012474746908992529)\n",
      "(341, 0.0011994283413514495)\n",
      "(342, 0.0011542895808815956)\n",
      "(343, 0.001112201833166182)\n",
      "(344, 0.0010705870809033513)\n",
      "(345, 0.0010307045886293054)\n",
      "(346, 0.0009935316629707813)\n",
      "(347, 0.0009580368641763926)\n",
      "(348, 0.0009237942285835743)\n",
      "(349, 0.0008914432837627828)\n",
      "(350, 0.0008588014170527458)\n",
      "(351, 0.0008285456569865346)\n",
      "(352, 0.0007992704631760716)\n",
      "(353, 0.0007713119848631322)\n",
      "(354, 0.0007448748219758272)\n",
      "(355, 0.0007198734674602747)\n",
      "(356, 0.0006949011003598571)\n",
      "(357, 0.0006713413167744875)\n",
      "(358, 0.0006494889385066926)\n",
      "(359, 0.0006282458198256791)\n",
      "(360, 0.0006072482792660594)\n",
      "(361, 0.0005872171022929251)\n",
      "(362, 0.0005675214342772961)\n",
      "(363, 0.0005489547620527446)\n",
      "(364, 0.000531664933077991)\n",
      "(365, 0.0005150614306330681)\n",
      "(366, 0.0004978752695024014)\n",
      "(367, 0.00048225029604509473)\n",
      "(368, 0.00046811887295916677)\n",
      "(369, 0.0004538010689429939)\n",
      "(370, 0.0004407590313348919)\n",
      "(371, 0.00042675010627135634)\n",
      "(372, 0.0004134351038374007)\n",
      "(373, 0.0004013818106614053)\n",
      "(374, 0.00038932645111344755)\n",
      "(375, 0.00037801670259796083)\n",
      "(376, 0.00036652444396167994)\n",
      "(377, 0.00035592977656051517)\n",
      "(378, 0.00034560548374429345)\n",
      "(379, 0.00033596454886719584)\n",
      "(380, 0.00032622553408145905)\n",
      "(381, 0.00031694199424237013)\n",
      "(382, 0.00030887481989338994)\n",
      "(383, 0.0002998121199198067)\n",
      "(384, 0.0002911572519224137)\n",
      "(385, 0.0002836455241777003)\n",
      "(386, 0.00027667402173392475)\n",
      "(387, 0.0002695013827178627)\n",
      "(388, 0.000261846580542624)\n",
      "(389, 0.00025502173230051994)\n",
      "(390, 0.0002480291877873242)\n",
      "(391, 0.000241564805037342)\n",
      "(392, 0.00023507805599365383)\n",
      "(393, 0.0002289353869855404)\n",
      "(394, 0.00022347421327140182)\n",
      "(395, 0.00021821580594405532)\n",
      "(396, 0.00021218025358393788)\n",
      "(397, 0.0002066752640530467)\n",
      "(398, 0.00020205217879265547)\n",
      "(399, 0.0001975559862330556)\n",
      "(400, 0.00019271318160463125)\n",
      "(401, 0.00018840358825400472)\n",
      "(402, 0.0001841780758695677)\n",
      "(403, 0.00017986312741413713)\n",
      "(404, 0.00017552061763126403)\n",
      "(405, 0.00017126643797382712)\n",
      "(406, 0.00016764510655775666)\n",
      "(407, 0.00016364958719350398)\n",
      "(408, 0.00016006114310584962)\n",
      "(409, 0.00015610302216373384)\n",
      "(410, 0.00015244643145706505)\n",
      "(411, 0.00014912261394783854)\n",
      "(412, 0.00014608263154514134)\n",
      "(413, 0.0001427987008355558)\n",
      "(414, 0.00013972801389172673)\n",
      "(415, 0.00013686265447176993)\n",
      "(416, 0.00013384519843384624)\n",
      "(417, 0.00013130826118867844)\n",
      "(418, 0.00012853463704232126)\n",
      "(419, 0.00012546582729555666)\n",
      "(420, 0.00012271146988496184)\n",
      "(421, 0.00012036283442284912)\n",
      "(422, 0.00011798719788203016)\n",
      "(423, 0.00011584033200051636)\n",
      "(424, 0.00011334398004692048)\n",
      "(425, 0.00011118162365164608)\n",
      "(426, 0.00010884870425797999)\n",
      "(427, 0.00010671986092347652)\n",
      "(428, 0.00010452621791046113)\n",
      "(429, 0.00010260539420414716)\n",
      "(430, 0.00010070543066831306)\n",
      "(431, 9.885501640383154e-05)\n",
      "(432, 9.706333366921172e-05)\n",
      "(433, 9.502121974946931e-05)\n",
      "(434, 9.327680163551122e-05)\n",
      "(435, 9.148851677309722e-05)\n",
      "(436, 8.986485772766173e-05)\n",
      "(437, 8.832604362396523e-05)\n",
      "(438, 8.713945135241374e-05)\n",
      "(439, 8.515485387761146e-05)\n",
      "(440, 8.365373651031405e-05)\n",
      "(441, 8.220317249651998e-05)\n",
      "(442, 8.069464820437133e-05)\n",
      "(443, 7.936936890473589e-05)\n",
      "(444, 7.792389078531414e-05)\n",
      "(445, 7.653531793039292e-05)\n",
      "(446, 7.527974230470136e-05)\n",
      "(447, 7.395519060082734e-05)\n",
      "(448, 7.283630839083344e-05)\n",
      "(449, 7.122159149730578e-05)\n",
      "(450, 7.019783515715972e-05)\n",
      "(451, 6.904713518451899e-05)\n",
      "(452, 6.782713899156079e-05)\n",
      "(453, 6.659388600382954e-05)\n",
      "(454, 6.539651076309383e-05)\n",
      "(455, 6.431306974263862e-05)\n",
      "(456, 6.344973371597007e-05)\n",
      "(457, 6.264170951908454e-05)\n",
      "(458, 6.143360224086791e-05)\n",
      "(459, 6.0369558923412114e-05)\n",
      "(460, 5.955999586149119e-05)\n",
      "(461, 5.8766592701431364e-05)\n",
      "(462, 5.789345959783532e-05)\n",
      "(463, 5.6991873861989006e-05)\n",
      "(464, 5.6202308769570664e-05)\n",
      "(465, 5.54978905711323e-05)\n",
      "(466, 5.4488722526002675e-05)\n",
      "(467, 5.3727861086372286e-05)\n",
      "(468, 5.3074705647304654e-05)\n",
      "(469, 5.228444570093416e-05)\n",
      "(470, 5.1534407248254865e-05)\n",
      "(471, 5.0477519835112616e-05)\n",
      "(472, 4.97702530992683e-05)\n",
      "(473, 4.9085479986388236e-05)\n",
      "(474, 4.853292921325192e-05)\n",
      "(475, 4.782978066941723e-05)\n",
      "(476, 4.715893737738952e-05)\n",
      "(477, 4.652432835428044e-05)\n",
      "(478, 4.578346124617383e-05)\n",
      "(479, 4.522060044109821e-05)\n",
      "(480, 4.4599462853511795e-05)\n",
      "(481, 4.39501236542128e-05)\n",
      "(482, 4.338678263593465e-05)\n",
      "(483, 4.267803160473704e-05)\n",
      "(484, 4.230168997310102e-05)\n",
      "(485, 4.155076021561399e-05)\n",
      "(486, 4.113670001970604e-05)\n",
      "(487, 4.063668893650174e-05)\n",
      "(488, 4.0124865336110815e-05)\n",
      "(489, 3.945792559534311e-05)\n",
      "(490, 3.90044879168272e-05)\n",
      "(491, 3.8635505916317925e-05)\n",
      "(492, 3.8121746911201626e-05)\n",
      "(493, 3.7681180401705205e-05)\n",
      "(494, 3.729584568645805e-05)\n",
      "(495, 3.6825778806814924e-05)\n",
      "(496, 3.634129461715929e-05)\n",
      "(497, 3.59825644409284e-05)\n",
      "(498, 3.559712058631703e-05)\n",
      "(499, 3.520344034768641e-05)\n"
     ]
    }
   ],
   "source": [
    "# pytorch tensor version\n",
    "%time\n",
    "import torch\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# N batch_size; D_in input dimention\n",
    "# H hidden dimention; D_out output dimention\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = torch.randn(N, D_in, dtype=dtype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=dtype, device=device)\n",
    "\n",
    "#randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(500):\n",
    "    #forward \n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(i, loss)\n",
    "    \n",
    "    #backprop\n",
    "    grad_y_pred = 2.0 * (y_pred -y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #update\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "###PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "(0, 30193284.0)\n",
      "(1, 27335972.0)\n",
      "(2, 28052356.0)\n",
      "(3, 28322944.0)\n",
      "(4, 25304272.0)\n",
      "(5, 19202206.0)\n",
      "(6, 12352478.0)\n",
      "(7, 7166758.0)\n",
      "(8, 4048828.0)\n",
      "(9, 2408390.75)\n",
      "(10, 1569851.0)\n",
      "(11, 1126226.75)\n",
      "(12, 870342.8125)\n",
      "(13, 706824.125)\n",
      "(14, 591754.0)\n",
      "(15, 504692.875)\n",
      "(16, 435502.5)\n",
      "(17, 378687.34375)\n",
      "(18, 331161.5)\n",
      "(19, 290890.96875)\n",
      "(20, 256522.5)\n",
      "(21, 227022.296875)\n",
      "(22, 201538.96875)\n",
      "(23, 179403.625)\n",
      "(24, 160113.953125)\n",
      "(25, 143255.34375)\n",
      "(26, 128471.296875)\n",
      "(27, 115463.15625)\n",
      "(28, 103986.8984375)\n",
      "(29, 93824.8359375)\n",
      "(30, 84810.6640625)\n",
      "(31, 76792.203125)\n",
      "(32, 69640.140625)\n",
      "(33, 63251.71875)\n",
      "(34, 57534.18359375)\n",
      "(35, 52406.546875)\n",
      "(36, 47802.8671875)\n",
      "(37, 43671.5859375)\n",
      "(38, 39946.09375)\n",
      "(39, 36581.796875)\n",
      "(40, 33537.5859375)\n",
      "(41, 30778.640625)\n",
      "(42, 28275.775390625)\n",
      "(43, 26001.205078125)\n",
      "(44, 23932.19140625)\n",
      "(45, 22048.978515625)\n",
      "(46, 20331.65234375)\n",
      "(47, 18762.380859375)\n",
      "(48, 17328.60546875)\n",
      "(49, 16017.8291015625)\n",
      "(50, 14818.27734375)\n",
      "(51, 13717.98046875)\n",
      "(52, 12708.7138671875)\n",
      "(53, 11782.7177734375)\n",
      "(54, 10931.845703125)\n",
      "(55, 10148.638671875)\n",
      "(56, 9427.5068359375)\n",
      "(57, 8763.0029296875)\n",
      "(58, 8150.1005859375)\n",
      "(59, 7584.23779296875)\n",
      "(60, 7061.662109375)\n",
      "(61, 6578.77392578125)\n",
      "(62, 6132.21728515625)\n",
      "(63, 5718.8466796875)\n",
      "(64, 5336.107421875)\n",
      "(65, 4981.4462890625)\n",
      "(66, 4652.53173828125)\n",
      "(67, 4347.35888671875)\n",
      "(68, 4064.22900390625)\n",
      "(69, 3801.373291015625)\n",
      "(70, 3557.123779296875)\n",
      "(71, 3329.977783203125)\n",
      "(72, 3118.767822265625)\n",
      "(73, 2922.14990234375)\n",
      "(74, 2739.0439453125)\n",
      "(75, 2568.43701171875)\n",
      "(76, 2409.31005859375)\n",
      "(77, 2260.8876953125)\n",
      "(78, 2122.385986328125)\n",
      "(79, 1993.0516357421875)\n",
      "(80, 1872.217529296875)\n",
      "(81, 1759.336669921875)\n",
      "(82, 1653.835205078125)\n",
      "(83, 1555.1075439453125)\n",
      "(84, 1462.7772216796875)\n",
      "(85, 1376.4200439453125)\n",
      "(86, 1295.5252685546875)\n",
      "(87, 1219.73486328125)\n",
      "(88, 1148.7242431640625)\n",
      "(89, 1082.112548828125)\n",
      "(90, 1019.667236328125)\n",
      "(91, 961.0845336914062)\n",
      "(92, 906.1278076171875)\n",
      "(93, 854.5655517578125)\n",
      "(94, 806.1449584960938)\n",
      "(95, 760.6387329101562)\n",
      "(96, 717.8807373046875)\n",
      "(97, 677.6948852539062)\n",
      "(98, 639.905029296875)\n",
      "(99, 604.352294921875)\n",
      "(100, 570.9125366210938)\n",
      "(101, 539.4391479492188)\n",
      "(102, 509.81494140625)\n",
      "(103, 481.917236328125)\n",
      "(104, 455.6372985839844)\n",
      "(105, 430.875244140625)\n",
      "(106, 407.5416564941406)\n",
      "(107, 385.591064453125)\n",
      "(108, 364.90386962890625)\n",
      "(109, 345.39093017578125)\n",
      "(110, 326.9824523925781)\n",
      "(111, 309.62237548828125)\n",
      "(112, 293.2374267578125)\n",
      "(113, 277.76904296875)\n",
      "(114, 263.1645202636719)\n",
      "(115, 249.37265014648438)\n",
      "(116, 236.33563232421875)\n",
      "(117, 224.0151824951172)\n",
      "(118, 212.3670196533203)\n",
      "(119, 201.3587646484375)\n",
      "(120, 190.95101928710938)\n",
      "(121, 181.10476684570312)\n",
      "(122, 171.7926788330078)\n",
      "(123, 162.98106384277344)\n",
      "(124, 154.64166259765625)\n",
      "(125, 146.75164794921875)\n",
      "(126, 139.2794189453125)\n",
      "(127, 132.2058868408203)\n",
      "(128, 125.51220703125)\n",
      "(129, 119.16571044921875)\n",
      "(130, 113.15740966796875)\n",
      "(131, 107.46566009521484)\n",
      "(132, 102.067626953125)\n",
      "(133, 96.95491790771484)\n",
      "(134, 92.109375)\n",
      "(135, 87.51553344726562)\n",
      "(136, 83.1599349975586)\n",
      "(137, 79.03022003173828)\n",
      "(138, 75.11383819580078)\n",
      "(139, 71.39784240722656)\n",
      "(140, 67.87548828125)\n",
      "(141, 64.53284454345703)\n",
      "(142, 61.35826110839844)\n",
      "(143, 58.34674835205078)\n",
      "(144, 55.48780059814453)\n",
      "(145, 52.77376174926758)\n",
      "(146, 50.19757843017578)\n",
      "(147, 47.75105285644531)\n",
      "(148, 45.428565979003906)\n",
      "(149, 43.22187805175781)\n",
      "(150, 41.125709533691406)\n",
      "(151, 39.134700775146484)\n",
      "(152, 37.24355697631836)\n",
      "(153, 35.447330474853516)\n",
      "(154, 33.73857879638672)\n",
      "(155, 32.115936279296875)\n",
      "(156, 30.573158264160156)\n",
      "(157, 29.106462478637695)\n",
      "(158, 27.712745666503906)\n",
      "(159, 26.38803482055664)\n",
      "(160, 25.127132415771484)\n",
      "(161, 23.92947006225586)\n",
      "(162, 22.788902282714844)\n",
      "(163, 21.705162048339844)\n",
      "(164, 20.674591064453125)\n",
      "(165, 19.695322036743164)\n",
      "(166, 18.76238250732422)\n",
      "(167, 17.874027252197266)\n",
      "(168, 17.029876708984375)\n",
      "(169, 16.226015090942383)\n",
      "(170, 15.461193084716797)\n",
      "(171, 14.733762741088867)\n",
      "(172, 14.040451049804688)\n",
      "(173, 13.381401062011719)\n",
      "(174, 12.753433227539062)\n",
      "(175, 12.156248092651367)\n",
      "(176, 11.587725639343262)\n",
      "(177, 11.046165466308594)\n",
      "(178, 10.530138969421387)\n",
      "(179, 10.039161682128906)\n",
      "(180, 9.571520805358887)\n",
      "(181, 9.126363754272461)\n",
      "(182, 8.701955795288086)\n",
      "(183, 8.297486305236816)\n",
      "(184, 7.912727355957031)\n",
      "(185, 7.545745849609375)\n",
      "(186, 7.196473121643066)\n",
      "(187, 6.863677024841309)\n",
      "(188, 6.546660423278809)\n",
      "(189, 6.244381904602051)\n",
      "(190, 5.956469535827637)\n",
      "(191, 5.6816792488098145)\n",
      "(192, 5.419853687286377)\n",
      "(193, 5.170954704284668)\n",
      "(194, 4.933472633361816)\n",
      "(195, 4.706826686859131)\n",
      "(196, 4.4911580085754395)\n",
      "(197, 4.285428524017334)\n",
      "(198, 4.089441299438477)\n",
      "(199, 3.902179718017578)\n",
      "(200, 3.723970651626587)\n",
      "(201, 3.553846597671509)\n",
      "(202, 3.391651153564453)\n",
      "(203, 3.237098217010498)\n",
      "(204, 3.0896027088165283)\n",
      "(205, 2.949047565460205)\n",
      "(206, 2.815080404281616)\n",
      "(207, 2.687168598175049)\n",
      "(208, 2.5651981830596924)\n",
      "(209, 2.4488158226013184)\n",
      "(210, 2.337761640548706)\n",
      "(211, 2.2320311069488525)\n",
      "(212, 2.1308321952819824)\n",
      "(213, 2.034550905227661)\n",
      "(214, 1.9426679611206055)\n",
      "(215, 1.8548457622528076)\n",
      "(216, 1.771122932434082)\n",
      "(217, 1.6912727355957031)\n",
      "(218, 1.615146517753601)\n",
      "(219, 1.5424747467041016)\n",
      "(220, 1.472968578338623)\n",
      "(221, 1.406792402267456)\n",
      "(222, 1.3436311483383179)\n",
      "(223, 1.2831716537475586)\n",
      "(224, 1.2256187200546265)\n",
      "(225, 1.1706912517547607)\n",
      "(226, 1.1181626319885254)\n",
      "(227, 1.0681703090667725)\n",
      "(228, 1.020288348197937)\n",
      "(229, 0.974784255027771)\n",
      "(230, 0.9312458038330078)\n",
      "(231, 0.8895956873893738)\n",
      "(232, 0.8499096632003784)\n",
      "(233, 0.8120082020759583)\n",
      "(234, 0.7758626937866211)\n",
      "(235, 0.7413076162338257)\n",
      "(236, 0.7082303166389465)\n",
      "(237, 0.6767368912696838)\n",
      "(238, 0.6466514468193054)\n",
      "(239, 0.6179554462432861)\n",
      "(240, 0.5904913544654846)\n",
      "(241, 0.5643133521080017)\n",
      "(242, 0.5391956567764282)\n",
      "(243, 0.5154341459274292)\n",
      "(244, 0.49253612756729126)\n",
      "(245, 0.4708109200000763)\n",
      "(246, 0.4499410390853882)\n",
      "(247, 0.43010812997817993)\n",
      "(248, 0.41106539964675903)\n",
      "(249, 0.3929290771484375)\n",
      "(250, 0.3755408525466919)\n",
      "(251, 0.35896986722946167)\n",
      "(252, 0.34316176176071167)\n",
      "(253, 0.32806655764579773)\n",
      "(254, 0.3136410117149353)\n",
      "(255, 0.299812376499176)\n",
      "(256, 0.2866227924823761)\n",
      "(257, 0.2740399241447449)\n",
      "(258, 0.26200252771377563)\n",
      "(259, 0.25048238039016724)\n",
      "(260, 0.2394554167985916)\n",
      "(261, 0.22898098826408386)\n",
      "(262, 0.2189215123653412)\n",
      "(263, 0.20935285091400146)\n",
      "(264, 0.20018833875656128)\n",
      "(265, 0.19141006469726562)\n",
      "(266, 0.18304213881492615)\n",
      "(267, 0.1750333458185196)\n",
      "(268, 0.16739709675312042)\n",
      "(269, 0.16009996831417084)\n",
      "(270, 0.15310873091220856)\n",
      "(271, 0.14643967151641846)\n",
      "(272, 0.14005286991596222)\n",
      "(273, 0.13392747938632965)\n",
      "(274, 0.1281033754348755)\n",
      "(275, 0.1224936991930008)\n",
      "(276, 0.11717187613248825)\n",
      "(277, 0.11207312345504761)\n",
      "(278, 0.1071905568242073)\n",
      "(279, 0.10254745185375214)\n",
      "(280, 0.09808847308158875)\n",
      "(281, 0.09382855892181396)\n",
      "(282, 0.08974133431911469)\n",
      "(283, 0.08587020635604858)\n",
      "(284, 0.08217085897922516)\n",
      "(285, 0.07860708236694336)\n",
      "(286, 0.0751875787973404)\n",
      "(287, 0.07194159179925919)\n",
      "(288, 0.06882847845554352)\n",
      "(289, 0.06584133952856064)\n",
      "(290, 0.0629962682723999)\n",
      "(291, 0.06028929725289345)\n",
      "(292, 0.057717423886060715)\n",
      "(293, 0.0552169531583786)\n",
      "(294, 0.05284636467695236)\n",
      "(295, 0.050550080835819244)\n",
      "(296, 0.04837318882346153)\n",
      "(297, 0.046303387731313705)\n",
      "(298, 0.044308852404356)\n",
      "(299, 0.04241892695426941)\n",
      "(300, 0.04058220610022545)\n",
      "(301, 0.03884740173816681)\n",
      "(302, 0.037175145000219345)\n",
      "(303, 0.03559550270438194)\n",
      "(304, 0.03407028689980507)\n",
      "(305, 0.032618265599012375)\n",
      "(306, 0.031218694522976875)\n",
      "(307, 0.029893403872847557)\n",
      "(308, 0.0286258477717638)\n",
      "(309, 0.027407050132751465)\n",
      "(310, 0.026236584410071373)\n",
      "(311, 0.025113582611083984)\n",
      "(312, 0.024036793038249016)\n",
      "(313, 0.023014962673187256)\n",
      "(314, 0.022036459296941757)\n",
      "(315, 0.02110539935529232)\n",
      "(316, 0.020223498344421387)\n",
      "(317, 0.019356537610292435)\n",
      "(318, 0.018535621464252472)\n",
      "(319, 0.017750080674886703)\n",
      "(320, 0.01700536534190178)\n",
      "(321, 0.016292273998260498)\n",
      "(322, 0.01560312882065773)\n",
      "(323, 0.014945746399462223)\n",
      "(324, 0.014325397089123726)\n",
      "(325, 0.013712430372834206)\n",
      "(326, 0.013144663535058498)\n",
      "(327, 0.012594202533364296)\n",
      "(328, 0.012073169462382793)\n",
      "(329, 0.011558507569134235)\n",
      "(330, 0.01108199916779995)\n",
      "(331, 0.0106285335496068)\n",
      "(332, 0.010189569555222988)\n",
      "(333, 0.009766435250639915)\n",
      "(334, 0.009359614923596382)\n",
      "(335, 0.008979191072285175)\n",
      "(336, 0.008614115417003632)\n",
      "(337, 0.008259490132331848)\n",
      "(338, 0.0079197334125638)\n",
      "(339, 0.00760170491412282)\n",
      "(340, 0.0072901686653494835)\n",
      "(341, 0.006991570815443993)\n",
      "(342, 0.0067102196626365185)\n",
      "(343, 0.006438225042074919)\n",
      "(344, 0.006177175790071487)\n",
      "(345, 0.005929047707468271)\n",
      "(346, 0.005690497346222401)\n",
      "(347, 0.005459409207105637)\n",
      "(348, 0.0052532595582306385)\n",
      "(349, 0.005043123383074999)\n",
      "(350, 0.0048448205925524235)\n",
      "(351, 0.0046540480107069016)\n",
      "(352, 0.00447459751740098)\n",
      "(353, 0.004297467879951)\n",
      "(354, 0.004130939021706581)\n",
      "(355, 0.003972790669649839)\n",
      "(356, 0.003825224470347166)\n",
      "(357, 0.0036803032271564007)\n",
      "(358, 0.003537599230185151)\n",
      "(359, 0.003402169793844223)\n",
      "(360, 0.0032767511438578367)\n",
      "(361, 0.0031551558058708906)\n",
      "(362, 0.0030377160292118788)\n",
      "(363, 0.0029237871058285236)\n",
      "(364, 0.00281090522184968)\n",
      "(365, 0.002709218766540289)\n",
      "(366, 0.0026070591993629932)\n",
      "(367, 0.002513298997655511)\n",
      "(368, 0.002419143682345748)\n",
      "(369, 0.0023323148488998413)\n",
      "(370, 0.002248103730380535)\n",
      "(371, 0.0021663892548531294)\n",
      "(372, 0.0020913940388709307)\n",
      "(373, 0.002018336672335863)\n",
      "(374, 0.0019465015502646565)\n",
      "(375, 0.0018797613447532058)\n",
      "(376, 0.0018136915750801563)\n",
      "(377, 0.001750635914504528)\n",
      "(378, 0.0016916858730837703)\n",
      "(379, 0.0016333293169736862)\n",
      "(380, 0.0015775394858792424)\n",
      "(381, 0.0015251452568918467)\n",
      "(382, 0.0014719187747687101)\n",
      "(383, 0.001424882560968399)\n",
      "(384, 0.0013783405302092433)\n",
      "(385, 0.001333547872491181)\n",
      "(386, 0.001289071748033166)\n",
      "(387, 0.0012467418564483523)\n",
      "(388, 0.0012088685762137175)\n",
      "(389, 0.0011677517322823405)\n",
      "(390, 0.001129961689002812)\n",
      "(391, 0.001097140833735466)\n",
      "(392, 0.001063111936673522)\n",
      "(393, 0.0010299060959368944)\n",
      "(394, 0.0009974805871024728)\n",
      "(395, 0.0009670106228441)\n",
      "(396, 0.0009363637072965503)\n",
      "(397, 0.0009093396365642548)\n",
      "(398, 0.0008823790703900158)\n",
      "(399, 0.0008570703212171793)\n",
      "(400, 0.0008295187726616859)\n",
      "(401, 0.0008048310410231352)\n",
      "(402, 0.0007818351150490344)\n",
      "(403, 0.0007597504882141948)\n",
      "(404, 0.000738243106752634)\n",
      "(405, 0.0007173864869400859)\n",
      "(406, 0.0006960501195862889)\n",
      "(407, 0.0006782232085242867)\n",
      "(408, 0.0006589207332581282)\n",
      "(409, 0.0006398812984116375)\n",
      "(410, 0.0006217193440534174)\n",
      "(411, 0.000605493551120162)\n",
      "(412, 0.0005894949426874518)\n",
      "(413, 0.000573868106584996)\n",
      "(414, 0.0005582021549344063)\n",
      "(415, 0.0005444245180115104)\n",
      "(416, 0.0005296739400364459)\n",
      "(417, 0.000515526975505054)\n",
      "(418, 0.0005036653601564467)\n",
      "(419, 0.0004894596058875322)\n",
      "(420, 0.0004775820707436651)\n",
      "(421, 0.00046615308383479714)\n",
      "(422, 0.00045416364446282387)\n",
      "(423, 0.00044250075006857514)\n",
      "(424, 0.00043202826054766774)\n",
      "(425, 0.00042064362787641585)\n",
      "(426, 0.0004110561276320368)\n",
      "(427, 0.00040071739931590855)\n",
      "(428, 0.0003904541954398155)\n",
      "(429, 0.00038178631803020835)\n",
      "(430, 0.00037240859819576144)\n",
      "(431, 0.0003639435162767768)\n",
      "(432, 0.00035563675919547677)\n",
      "(433, 0.00034739437978714705)\n",
      "(434, 0.00034004574990831316)\n",
      "(435, 0.0003312729822937399)\n",
      "(436, 0.00032500765519216657)\n",
      "(437, 0.00031761181890033185)\n",
      "(438, 0.00030975326080806553)\n",
      "(439, 0.0003028616774827242)\n",
      "(440, 0.0002962216967716813)\n",
      "(441, 0.0002901690313592553)\n",
      "(442, 0.00028293943614698946)\n",
      "(443, 0.00027693877927958965)\n",
      "(444, 0.0002713928115554154)\n",
      "(445, 0.0002655019052326679)\n",
      "(446, 0.0002601595188025385)\n",
      "(447, 0.00025487702805548906)\n",
      "(448, 0.00024963682517409325)\n",
      "(449, 0.00024461865541525185)\n",
      "(450, 0.0002389925066381693)\n",
      "(451, 0.00023448708816431463)\n",
      "(452, 0.00023054209304973483)\n",
      "(453, 0.00022555410396307707)\n",
      "(454, 0.00022120137873571366)\n",
      "(455, 0.00021688481501769274)\n",
      "(456, 0.00021197661408223212)\n",
      "(457, 0.0002083057479467243)\n",
      "(458, 0.00020426703849807382)\n",
      "(459, 0.0002005134301725775)\n",
      "(460, 0.00019717015675269067)\n",
      "(461, 0.00019290328782517463)\n",
      "(462, 0.00018960444140248)\n",
      "(463, 0.00018597938469611108)\n",
      "(464, 0.0001825700601330027)\n",
      "(465, 0.00017952913185581565)\n",
      "(466, 0.0001761473249644041)\n",
      "(467, 0.00017309085524175316)\n",
      "(468, 0.00016994743782561272)\n",
      "(469, 0.0001672743819653988)\n",
      "(470, 0.00016394187696278095)\n",
      "(471, 0.00016147748101502657)\n",
      "(472, 0.00015805693692527711)\n",
      "(473, 0.00015531441022176296)\n",
      "(474, 0.0001523063547210768)\n",
      "(475, 0.00014961979468353093)\n",
      "(476, 0.0001472307340009138)\n",
      "(477, 0.00014511507470160723)\n",
      "(478, 0.0001422873028786853)\n",
      "(479, 0.0001398446038365364)\n",
      "(480, 0.00013792749086860567)\n",
      "(481, 0.00013559435319621116)\n",
      "(482, 0.00013305274478625506)\n",
      "(483, 0.00013087985280435532)\n",
      "(484, 0.00012863556912634522)\n",
      "(485, 0.00012684417015407234)\n",
      "(486, 0.00012494927796069533)\n",
      "(487, 0.00012313992192503065)\n",
      "(488, 0.00012125165812904015)\n",
      "(489, 0.00011932128109037876)\n",
      "(490, 0.00011762529175030068)\n",
      "(491, 0.00011592770169954747)\n",
      "(492, 0.0001139269006671384)\n",
      "(493, 0.00011210342199774459)\n",
      "(494, 0.00011061596887884662)\n",
      "(495, 0.00010872781422222033)\n",
      "(496, 0.0001070462167263031)\n",
      "(497, 0.00010612231562845409)\n",
      "(498, 0.00010450620902702212)\n",
      "(499, 0.00010320439469069242)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "# N batch_size; D_in input dimention\n",
    "# H hidden dimention; D_out output dimention\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred -y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # manually zero all the gradient after update weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PyTorch: Defining new autograd functions\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "(0, 36068840.0)\n",
      "(1, 29475204.0)\n",
      "(2, 24179312.0)\n",
      "(3, 18218820.0)\n",
      "(4, 12399582.0)\n",
      "(5, 7888791.0)\n",
      "(6, 4953786.0)\n",
      "(7, 3225837.0)\n",
      "(8, 2235222.0)\n",
      "(9, 1651326.375)\n",
      "(10, 1285989.125)\n",
      "(11, 1040177.0625)\n",
      "(12, 863155.5625)\n",
      "(13, 728682.125)\n",
      "(14, 622616.75)\n",
      "(15, 536783.875)\n",
      "(16, 465761.15625)\n",
      "(17, 406337.28125)\n",
      "(18, 356146.0)\n",
      "(19, 313385.375)\n",
      "(20, 276768.78125)\n",
      "(21, 245231.28125)\n",
      "(22, 217905.1875)\n",
      "(23, 194143.03125)\n",
      "(24, 173402.59375)\n",
      "(25, 155237.453125)\n",
      "(26, 139298.6875)\n",
      "(27, 125304.0078125)\n",
      "(28, 112942.765625)\n",
      "(29, 101984.7578125)\n",
      "(30, 92254.5859375)\n",
      "(31, 83593.265625)\n",
      "(32, 75880.328125)\n",
      "(33, 68984.8515625)\n",
      "(34, 62803.94921875)\n",
      "(35, 57254.578125)\n",
      "(36, 52261.5859375)\n",
      "(37, 47762.7734375)\n",
      "(38, 43705.13671875)\n",
      "(39, 40037.20703125)\n",
      "(40, 36717.59375)\n",
      "(41, 33710.26171875)\n",
      "(42, 30982.220703125)\n",
      "(43, 28501.251953125)\n",
      "(44, 26246.091796875)\n",
      "(45, 24192.4375)\n",
      "(46, 22322.453125)\n",
      "(47, 20615.04296875)\n",
      "(48, 19054.7734375)\n",
      "(49, 17626.208984375)\n",
      "(50, 16317.056640625)\n",
      "(51, 15116.5263671875)\n",
      "(52, 14014.572265625)\n",
      "(53, 13003.0712890625)\n",
      "(54, 12072.859375)\n",
      "(55, 11216.328125)\n",
      "(56, 10427.4814453125)\n",
      "(57, 9700.6552734375)\n",
      "(58, 9030.32421875)\n",
      "(59, 8411.3671875)\n",
      "(60, 7839.4423828125)\n",
      "(61, 7310.857421875)\n",
      "(62, 6821.78125)\n",
      "(63, 6369.44140625)\n",
      "(64, 5950.486328125)\n",
      "(65, 5562.0732421875)\n",
      "(66, 5203.40234375)\n",
      "(67, 4872.0654296875)\n",
      "(68, 4564.2119140625)\n",
      "(69, 4278.19091796875)\n",
      "(70, 4011.95703125)\n",
      "(71, 3763.97412109375)\n",
      "(72, 3533.05615234375)\n",
      "(73, 3317.880859375)\n",
      "(74, 3117.18994140625)\n",
      "(75, 2929.899169921875)\n",
      "(76, 2755.101318359375)\n",
      "(77, 2591.784912109375)\n",
      "(78, 2439.169921875)\n",
      "(79, 2296.45703125)\n",
      "(80, 2162.955078125)\n",
      "(81, 2037.989501953125)\n",
      "(82, 1920.954345703125)\n",
      "(83, 1811.273681640625)\n",
      "(84, 1708.458251953125)\n",
      "(85, 1612.0401611328125)\n",
      "(86, 1521.581787109375)\n",
      "(87, 1436.693603515625)\n",
      "(88, 1356.981689453125)\n",
      "(89, 1282.0927734375)\n",
      "(90, 1211.7177734375)\n",
      "(91, 1145.55078125)\n",
      "(92, 1083.33154296875)\n",
      "(93, 1024.8067626953125)\n",
      "(94, 969.7280883789062)\n",
      "(95, 917.88232421875)\n",
      "(96, 869.03466796875)\n",
      "(97, 823.0156860351562)\n",
      "(98, 779.64404296875)\n",
      "(99, 738.7540893554688)\n",
      "(100, 700.2041015625)\n",
      "(101, 663.83544921875)\n",
      "(102, 629.5050659179688)\n",
      "(103, 597.093017578125)\n",
      "(104, 566.4915771484375)\n",
      "(105, 537.5917358398438)\n",
      "(106, 510.2823791503906)\n",
      "(107, 484.46697998046875)\n",
      "(108, 460.0705261230469)\n",
      "(109, 436.9925537109375)\n",
      "(110, 415.1690673828125)\n",
      "(111, 394.5211486816406)\n",
      "(112, 374.97296142578125)\n",
      "(113, 356.46929931640625)\n",
      "(114, 338.94805908203125)\n",
      "(115, 322.3564453125)\n",
      "(116, 306.6326904296875)\n",
      "(117, 291.73602294921875)\n",
      "(118, 277.61163330078125)\n",
      "(119, 264.2236328125)\n",
      "(120, 251.5277099609375)\n",
      "(121, 239.48095703125)\n",
      "(122, 228.0559844970703)\n",
      "(123, 217.2115936279297)\n",
      "(124, 206.91957092285156)\n",
      "(125, 197.1473388671875)\n",
      "(126, 187.86614990234375)\n",
      "(127, 179.0544891357422)\n",
      "(128, 170.6809844970703)\n",
      "(129, 162.73007202148438)\n",
      "(130, 155.17282104492188)\n",
      "(131, 147.98678588867188)\n",
      "(132, 141.15963745117188)\n",
      "(133, 134.6627655029297)\n",
      "(134, 128.4835662841797)\n",
      "(135, 122.60706329345703)\n",
      "(136, 117.01353454589844)\n",
      "(137, 111.69200897216797)\n",
      "(138, 106.62760925292969)\n",
      "(139, 101.8074951171875)\n",
      "(140, 97.21707153320312)\n",
      "(141, 92.84355926513672)\n",
      "(142, 88.67903900146484)\n",
      "(143, 84.7118911743164)\n",
      "(144, 80.93243408203125)\n",
      "(145, 77.33004760742188)\n",
      "(146, 73.89765167236328)\n",
      "(147, 70.6264419555664)\n",
      "(148, 67.5064468383789)\n",
      "(149, 64.53187561035156)\n",
      "(150, 61.69573974609375)\n",
      "(151, 58.990196228027344)\n",
      "(152, 56.410362243652344)\n",
      "(153, 53.94736862182617)\n",
      "(154, 51.59844207763672)\n",
      "(155, 49.35624313354492)\n",
      "(156, 47.216461181640625)\n",
      "(157, 45.17372131347656)\n",
      "(158, 43.22370910644531)\n",
      "(159, 41.36200714111328)\n",
      "(160, 39.58441162109375)\n",
      "(161, 37.88671112060547)\n",
      "(162, 36.26411056518555)\n",
      "(163, 34.71565246582031)\n",
      "(164, 33.23518371582031)\n",
      "(165, 31.82071876525879)\n",
      "(166, 30.46988868713379)\n",
      "(167, 29.178359985351562)\n",
      "(168, 27.94369888305664)\n",
      "(169, 26.76374626159668)\n",
      "(170, 25.635160446166992)\n",
      "(171, 24.55620574951172)\n",
      "(172, 23.524864196777344)\n",
      "(173, 22.538501739501953)\n",
      "(174, 21.594589233398438)\n",
      "(175, 20.692169189453125)\n",
      "(176, 19.829248428344727)\n",
      "(177, 19.003278732299805)\n",
      "(178, 18.212921142578125)\n",
      "(179, 17.456819534301758)\n",
      "(180, 16.73345375061035)\n",
      "(181, 16.040821075439453)\n",
      "(182, 15.377941131591797)\n",
      "(183, 14.743749618530273)\n",
      "(184, 14.136392593383789)\n",
      "(185, 13.554443359375)\n",
      "(186, 12.997929573059082)\n",
      "(187, 12.464466094970703)\n",
      "(188, 11.954078674316406)\n",
      "(189, 11.465087890625)\n",
      "(190, 10.996728897094727)\n",
      "(191, 10.548116683959961)\n",
      "(192, 10.118387222290039)\n",
      "(193, 9.706633567810059)\n",
      "(194, 9.312193870544434)\n",
      "(195, 8.934610366821289)\n",
      "(196, 8.572589874267578)\n",
      "(197, 8.22581672668457)\n",
      "(198, 7.893165588378906)\n",
      "(199, 7.574570655822754)\n",
      "(200, 7.26892614364624)\n",
      "(201, 6.976140975952148)\n",
      "(202, 6.695413589477539)\n",
      "(203, 6.426191329956055)\n",
      "(204, 6.168387413024902)\n",
      "(205, 5.921009540557861)\n",
      "(206, 5.6839141845703125)\n",
      "(207, 5.456447124481201)\n",
      "(208, 5.2383575439453125)\n",
      "(209, 5.0291266441345215)\n",
      "(210, 4.828507423400879)\n",
      "(211, 4.6361918449401855)\n",
      "(212, 4.451747894287109)\n",
      "(213, 4.274683475494385)\n",
      "(214, 4.104910850524902)\n",
      "(215, 3.9419357776641846)\n",
      "(216, 3.7855329513549805)\n",
      "(217, 3.635692596435547)\n",
      "(218, 3.491762638092041)\n",
      "(219, 3.3536887168884277)\n",
      "(220, 3.221371650695801)\n",
      "(221, 3.0941109657287598)\n",
      "(222, 2.9720821380615234)\n",
      "(223, 2.8548731803894043)\n",
      "(224, 2.742649555206299)\n",
      "(225, 2.63480281829834)\n",
      "(226, 2.531177043914795)\n",
      "(227, 2.4318180084228516)\n",
      "(228, 2.3364768028259277)\n",
      "(229, 2.245049476623535)\n",
      "(230, 2.1568374633789062)\n",
      "(231, 2.0724852085113525)\n",
      "(232, 1.9914287328720093)\n",
      "(233, 1.9137117862701416)\n",
      "(234, 1.8388766050338745)\n",
      "(235, 1.767146348953247)\n",
      "(236, 1.6982858180999756)\n",
      "(237, 1.6321521997451782)\n",
      "(238, 1.5685670375823975)\n",
      "(239, 1.5075221061706543)\n",
      "(240, 1.4489315748214722)\n",
      "(241, 1.3926632404327393)\n",
      "(242, 1.3385353088378906)\n",
      "(243, 1.2866694927215576)\n",
      "(244, 1.2368100881576538)\n",
      "(245, 1.1887435913085938)\n",
      "(246, 1.1428203582763672)\n",
      "(247, 1.0985405445098877)\n",
      "(248, 1.0559674501419067)\n",
      "(249, 1.0151838064193726)\n",
      "(250, 0.975969672203064)\n",
      "(251, 0.9382502436637878)\n",
      "(252, 0.9020841121673584)\n",
      "(253, 0.8672796487808228)\n",
      "(254, 0.8339381217956543)\n",
      "(255, 0.8017646670341492)\n",
      "(256, 0.7709072828292847)\n",
      "(257, 0.7412474155426025)\n",
      "(258, 0.7127498388290405)\n",
      "(259, 0.685409426689148)\n",
      "(260, 0.6590099334716797)\n",
      "(261, 0.6337711811065674)\n",
      "(262, 0.6094098091125488)\n",
      "(263, 0.5861008167266846)\n",
      "(264, 0.5635946989059448)\n",
      "(265, 0.5420149564743042)\n",
      "(266, 0.521247386932373)\n",
      "(267, 0.5013443827629089)\n",
      "(268, 0.48210418224334717)\n",
      "(269, 0.4636399745941162)\n",
      "(270, 0.44591397047042847)\n",
      "(271, 0.42886221408843994)\n",
      "(272, 0.4125436842441559)\n",
      "(273, 0.39678114652633667)\n",
      "(274, 0.3816966414451599)\n",
      "(275, 0.3671475946903229)\n",
      "(276, 0.35315829515457153)\n",
      "(277, 0.33974489569664)\n",
      "(278, 0.32680216431617737)\n",
      "(279, 0.31438007950782776)\n",
      "(280, 0.3024488091468811)\n",
      "(281, 0.29093146324157715)\n",
      "(282, 0.27985483407974243)\n",
      "(283, 0.26923590898513794)\n",
      "(284, 0.2590459883213043)\n",
      "(285, 0.2492009699344635)\n",
      "(286, 0.23975235223770142)\n",
      "(287, 0.2306535243988037)\n",
      "(288, 0.22192297875881195)\n",
      "(289, 0.21350091695785522)\n",
      "(290, 0.20541530847549438)\n",
      "(291, 0.19761982560157776)\n",
      "(292, 0.190155491232872)\n",
      "(293, 0.1829315572977066)\n",
      "(294, 0.176060751080513)\n",
      "(295, 0.16940078139305115)\n",
      "(296, 0.16299140453338623)\n",
      "(297, 0.15682768821716309)\n",
      "(298, 0.1509009152650833)\n",
      "(299, 0.14523176848888397)\n",
      "(300, 0.13973096013069153)\n",
      "(301, 0.13445599377155304)\n",
      "(302, 0.12939316034317017)\n",
      "(303, 0.12451969087123871)\n",
      "(304, 0.11983829736709595)\n",
      "(305, 0.11533297598361969)\n",
      "(306, 0.11100298911333084)\n",
      "(307, 0.10681973397731781)\n",
      "(308, 0.10280323773622513)\n",
      "(309, 0.09892993420362473)\n",
      "(310, 0.09522663801908493)\n",
      "(311, 0.09163220971822739)\n",
      "(312, 0.08821098506450653)\n",
      "(313, 0.08489040285348892)\n",
      "(314, 0.08170716464519501)\n",
      "(315, 0.0786719024181366)\n",
      "(316, 0.07571443915367126)\n",
      "(317, 0.07285033166408539)\n",
      "(318, 0.0701521784067154)\n",
      "(319, 0.06750819087028503)\n",
      "(320, 0.06497960537672043)\n",
      "(321, 0.06255603581666946)\n",
      "(322, 0.060222554951906204)\n",
      "(323, 0.05795897915959358)\n",
      "(324, 0.05579939857125282)\n",
      "(325, 0.05372312664985657)\n",
      "(326, 0.05170486494898796)\n",
      "(327, 0.0497707799077034)\n",
      "(328, 0.04792063683271408)\n",
      "(329, 0.046127498149871826)\n",
      "(330, 0.044405195862054825)\n",
      "(331, 0.04276188462972641)\n",
      "(332, 0.04116835445165634)\n",
      "(333, 0.03963194787502289)\n",
      "(334, 0.03815223276615143)\n",
      "(335, 0.03673175722360611)\n",
      "(336, 0.03537146374583244)\n",
      "(337, 0.034060847014188766)\n",
      "(338, 0.03280137479305267)\n",
      "(339, 0.03158598393201828)\n",
      "(340, 0.030426915735006332)\n",
      "(341, 0.029294680804014206)\n",
      "(342, 0.028211358934640884)\n",
      "(343, 0.02716674655675888)\n",
      "(344, 0.02616117335855961)\n",
      "(345, 0.025207193568348885)\n",
      "(346, 0.02427764981985092)\n",
      "(347, 0.02337646484375)\n",
      "(348, 0.02250710502266884)\n",
      "(349, 0.021679196506738663)\n",
      "(350, 0.020889757201075554)\n",
      "(351, 0.02012503892183304)\n",
      "(352, 0.01938144490122795)\n",
      "(353, 0.018673300743103027)\n",
      "(354, 0.017989903688430786)\n",
      "(355, 0.017334459349513054)\n",
      "(356, 0.01669042557477951)\n",
      "(357, 0.01608394831418991)\n",
      "(358, 0.015487867407500744)\n",
      "(359, 0.014934230595827103)\n",
      "(360, 0.014392727985978127)\n",
      "(361, 0.013871492817997932)\n",
      "(362, 0.01335995178669691)\n",
      "(363, 0.012877743691205978)\n",
      "(364, 0.012411906383931637)\n",
      "(365, 0.01195547729730606)\n",
      "(366, 0.011529342271387577)\n",
      "(367, 0.011118704453110695)\n",
      "(368, 0.010720862075686455)\n",
      "(369, 0.010334393940865993)\n",
      "(370, 0.009960457682609558)\n",
      "(371, 0.009599427692592144)\n",
      "(372, 0.009252214804291725)\n",
      "(373, 0.008926200680434704)\n",
      "(374, 0.008608495816588402)\n",
      "(375, 0.008306244388222694)\n",
      "(376, 0.008007013238966465)\n",
      "(377, 0.007719200104475021)\n",
      "(378, 0.007447825279086828)\n",
      "(379, 0.007181433029472828)\n",
      "(380, 0.006933306343853474)\n",
      "(381, 0.006684684660285711)\n",
      "(382, 0.006447844207286835)\n",
      "(383, 0.006222591269761324)\n",
      "(384, 0.006002267822623253)\n",
      "(385, 0.005792836658656597)\n",
      "(386, 0.005589976441115141)\n",
      "(387, 0.005398253910243511)\n",
      "(388, 0.0052091763354837894)\n",
      "(389, 0.005028232000768185)\n",
      "(390, 0.004851398523896933)\n",
      "(391, 0.0046868231147527695)\n",
      "(392, 0.004525057040154934)\n",
      "(393, 0.004369743168354034)\n",
      "(394, 0.004222400486469269)\n",
      "(395, 0.004076466895639896)\n",
      "(396, 0.003940288908779621)\n",
      "(397, 0.0038070823065936565)\n",
      "(398, 0.0036795660853385925)\n",
      "(399, 0.00355285732075572)\n",
      "(400, 0.0034340606071054935)\n",
      "(401, 0.0033169337548315525)\n",
      "(402, 0.0032085746061056852)\n",
      "(403, 0.0031034471467137337)\n",
      "(404, 0.0029983590357005596)\n",
      "(405, 0.002901453757658601)\n",
      "(406, 0.00280573358759284)\n",
      "(407, 0.002713155932724476)\n",
      "(408, 0.002623738721013069)\n",
      "(409, 0.002541321562603116)\n",
      "(410, 0.0024597991723567247)\n",
      "(411, 0.0023799180053174496)\n",
      "(412, 0.002302376553416252)\n",
      "(413, 0.0022285785526037216)\n",
      "(414, 0.00215821317397058)\n",
      "(415, 0.0020899418741464615)\n",
      "(416, 0.00202372414059937)\n",
      "(417, 0.001962055219337344)\n",
      "(418, 0.0019017062149941921)\n",
      "(419, 0.0018404887523502111)\n",
      "(420, 0.0017847568960860372)\n",
      "(421, 0.001728099538013339)\n",
      "(422, 0.0016738628037273884)\n",
      "(423, 0.0016235900111496449)\n",
      "(424, 0.0015727330464869738)\n",
      "(425, 0.0015246747061610222)\n",
      "(426, 0.0014782007783651352)\n",
      "(427, 0.001434418372809887)\n",
      "(428, 0.0013896818272769451)\n",
      "(429, 0.0013484949013218284)\n",
      "(430, 0.0013079693308100104)\n",
      "(431, 0.0012694108299911022)\n",
      "(432, 0.001233558519743383)\n",
      "(433, 0.0011988626793026924)\n",
      "(434, 0.0011634484399110079)\n",
      "(435, 0.0011299998732283711)\n",
      "(436, 0.0010977612109854817)\n",
      "(437, 0.0010656669037416577)\n",
      "(438, 0.0010366869391873479)\n",
      "(439, 0.0010063922964036465)\n",
      "(440, 0.00097829254809767)\n",
      "(441, 0.0009508285438641906)\n",
      "(442, 0.0009234102908521891)\n",
      "(443, 0.0008984737796708941)\n",
      "(444, 0.000874697114340961)\n",
      "(445, 0.000850255717523396)\n",
      "(446, 0.0008281234186142683)\n",
      "(447, 0.0008052837220020592)\n",
      "(448, 0.0007836038712412119)\n",
      "(449, 0.0007627158774994314)\n",
      "(450, 0.0007433161372318864)\n",
      "(451, 0.0007249036571010947)\n",
      "(452, 0.0007058013579808176)\n",
      "(453, 0.0006870388751849532)\n",
      "(454, 0.0006689878064207733)\n",
      "(455, 0.000652498914860189)\n",
      "(456, 0.0006347413873299956)\n",
      "(457, 0.0006184869562275708)\n",
      "(458, 0.0006028115749359131)\n",
      "(459, 0.0005869789747521281)\n",
      "(460, 0.0005734032019972801)\n",
      "(461, 0.0005597182316705585)\n",
      "(462, 0.0005456312792375684)\n",
      "(463, 0.0005318333860486746)\n",
      "(464, 0.0005193088436499238)\n",
      "(465, 0.000506536103785038)\n",
      "(466, 0.0004952498711645603)\n",
      "(467, 0.0004817976150661707)\n",
      "(468, 0.00046996830496937037)\n",
      "(469, 0.00045863547711633146)\n",
      "(470, 0.00044785847421735525)\n",
      "(471, 0.00043777801329270005)\n",
      "(472, 0.0004276825347915292)\n",
      "(473, 0.0004184916615486145)\n",
      "(474, 0.00040793948573991656)\n",
      "(475, 0.0003988277749158442)\n",
      "(476, 0.00038975439383648336)\n",
      "(477, 0.00038051349110901356)\n",
      "(478, 0.00037096289452165365)\n",
      "(479, 0.0003624992386903614)\n",
      "(480, 0.0003548077365849167)\n",
      "(481, 0.00034700060496106744)\n",
      "(482, 0.0003404291928745806)\n",
      "(483, 0.00033297674963250756)\n",
      "(484, 0.00032526542781852186)\n",
      "(485, 0.00031841330928727984)\n",
      "(486, 0.00031159367063082755)\n",
      "(487, 0.0003048279613722116)\n",
      "(488, 0.0002984250313602388)\n",
      "(489, 0.0002922573476098478)\n",
      "(490, 0.0002860669628717005)\n",
      "(491, 0.00028040006873197854)\n",
      "(492, 0.00027517107082530856)\n",
      "(493, 0.00026968703605234623)\n",
      "(494, 0.00026398120098747313)\n",
      "(495, 0.00025930634001269937)\n",
      "(496, 0.00025364395696669817)\n",
      "(497, 0.00024762723478488624)\n",
      "(498, 0.00024343153927475214)\n",
      "(499, 0.00023855018662288785)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TensorFlow: Static Graphs\n",
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlow’s computational graphs are static and PyTorch uses dynamic computational graphs.\n",
    "\n",
    "In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "Static graphs are nice because you can optimize the graph up front; for example a framework might decide to fuse some graph operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over.\n",
    "\n",
    "One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "To contrast with the PyTorch autograd example above, here we use TensorFlow to fit a simple two-layer net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "22274388.0\n",
      "15002860.0\n",
      "11281076.0\n",
      "9100265.0\n",
      "7649930.5\n",
      "6556337.0\n",
      "5627670.0\n",
      "4799289.0\n",
      "4037848.8\n",
      "3353943.8\n",
      "2748241.5\n",
      "2230587.5\n",
      "1796526.0\n",
      "1442284.9\n",
      "1156499.2\n",
      "929592.4\n",
      "750114.5\n",
      "609045.1\n",
      "497972.16\n",
      "410527.0\n",
      "341335.06\n",
      "286314.12\n",
      "242269.66\n",
      "206747.33\n",
      "177808.73\n",
      "154018.16\n",
      "134281.25\n",
      "117756.92\n",
      "103804.7\n",
      "91932.45\n",
      "81762.75\n",
      "72995.73\n",
      "65386.855\n",
      "58740.285\n",
      "52905.68\n",
      "47766.715\n",
      "43213.14\n",
      "39163.043\n",
      "35549.836\n",
      "32317.54\n",
      "29418.734\n",
      "26813.191\n",
      "24466.607\n",
      "22352.621\n",
      "20441.47\n",
      "18711.184\n",
      "17142.775\n",
      "15719.33\n",
      "14426.096\n",
      "13249.576\n",
      "12178.383\n",
      "11201.654\n",
      "10311.056\n",
      "9497.358\n",
      "8753.814\n",
      "8073.377\n",
      "7450.2637\n",
      "6879.132\n",
      "6355.37\n",
      "5874.731\n",
      "5433.3213\n",
      "5027.7363\n",
      "4654.9336\n",
      "4311.8564\n",
      "3995.8513\n",
      "3704.9524\n",
      "3436.8489\n",
      "3189.5007\n",
      "2961.295\n",
      "2750.6936\n",
      "2556.1414\n",
      "2376.356\n",
      "2210.142\n",
      "2056.4368\n",
      "1914.1514\n",
      "1782.4141\n",
      "1660.4688\n",
      "1547.3923\n",
      "1442.5629\n",
      "1345.338\n",
      "1255.0876\n",
      "1171.3384\n",
      "1093.6149\n",
      "1021.376\n",
      "954.2449\n",
      "891.87787\n",
      "833.8183\n",
      "779.7829\n",
      "729.51685\n",
      "682.7002\n",
      "639.0997\n",
      "598.4525\n",
      "560.56287\n",
      "525.24805\n",
      "492.30038\n",
      "461.53522\n",
      "432.81537\n",
      "406.00464\n",
      "380.9638\n",
      "357.56012\n",
      "335.6906\n",
      "315.2442\n",
      "296.10916\n",
      "278.2158\n",
      "261.46442\n",
      "245.78607\n",
      "231.10959\n",
      "217.36995\n",
      "204.49382\n",
      "192.43188\n",
      "181.12845\n",
      "170.52023\n",
      "160.56873\n",
      "151.2361\n",
      "142.4757\n",
      "134.24875\n",
      "126.52436\n",
      "119.27064\n",
      "112.45321\n",
      "106.044815\n",
      "100.02246\n",
      "94.36026\n",
      "89.03607\n",
      "84.026825\n",
      "79.314354\n",
      "74.88199\n",
      "70.70797\n",
      "66.77781\n",
      "63.07639\n",
      "59.590836\n",
      "56.30653\n",
      "53.21125\n",
      "50.294563\n",
      "47.54741\n",
      "44.955376\n",
      "42.510983\n",
      "40.207146\n",
      "38.031998\n",
      "35.97979\n",
      "34.04322\n",
      "32.215614\n",
      "30.49104\n",
      "28.8613\n",
      "27.322556\n",
      "25.869274\n",
      "24.497091\n",
      "23.199322\n",
      "21.97349\n",
      "20.815369\n",
      "19.720585\n",
      "18.684994\n",
      "17.705795\n",
      "16.780184\n",
      "15.904591\n",
      "15.075776\n",
      "14.292674\n",
      "13.550844\n",
      "12.849443\n",
      "12.184911\n",
      "11.556894\n",
      "10.961445\n",
      "10.397618\n",
      "9.864225\n",
      "9.358764\n",
      "8.880007\n",
      "8.426357\n",
      "7.9969883\n",
      "7.5898075\n",
      "7.203704\n",
      "6.838038\n",
      "6.4917765\n",
      "6.1631765\n",
      "5.8519263\n",
      "5.556783\n",
      "5.2768755\n",
      "5.011423\n",
      "4.759529\n",
      "4.520591\n",
      "4.2939596\n",
      "4.079282\n",
      "3.8754575\n",
      "3.6818213\n",
      "3.4981627\n",
      "3.3239353\n",
      "3.1586742\n",
      "3.0018194\n",
      "2.852938\n",
      "2.7114232\n",
      "2.57721\n",
      "2.4497087\n",
      "2.3286371\n",
      "2.21369\n",
      "2.1045783\n",
      "2.001001\n",
      "1.9022259\n",
      "1.8088942\n",
      "1.7200489\n",
      "1.635612\n",
      "1.555631\n",
      "1.4793209\n",
      "1.4070705\n",
      "1.3382521\n",
      "1.2729175\n",
      "1.2107382\n",
      "1.15189\n",
      "1.0956637\n",
      "1.0424552\n",
      "0.9916639\n",
      "0.94347703\n",
      "0.8975937\n",
      "0.85403144\n",
      "0.81266475\n",
      "0.77331156\n",
      "0.735872\n",
      "0.70024335\n",
      "0.6664113\n",
      "0.6341559\n",
      "0.6035048\n",
      "0.5744257\n",
      "0.5468071\n",
      "0.5203854\n",
      "0.49535248\n",
      "0.47154164\n",
      "0.44885302\n",
      "0.42727086\n",
      "0.40674758\n",
      "0.38722348\n",
      "0.36866415\n",
      "0.35102463\n",
      "0.3342663\n",
      "0.31821\n",
      "0.30295736\n",
      "0.28844735\n",
      "0.27464983\n",
      "0.26154822\n",
      "0.24905336\n",
      "0.23716745\n",
      "0.22586983\n",
      "0.21509561\n",
      "0.20484978\n",
      "0.19509451\n",
      "0.1858028\n",
      "0.17697407\n",
      "0.1685594\n",
      "0.16048843\n",
      "0.1529007\n",
      "0.1456294\n",
      "0.13870633\n",
      "0.13215174\n",
      "0.12586693\n",
      "0.11990283\n",
      "0.11423714\n",
      "0.10881196\n",
      "0.10362159\n",
      "0.0987151\n",
      "0.09403841\n",
      "0.08959602\n",
      "0.08538422\n",
      "0.08131888\n",
      "0.077512965\n",
      "0.07384735\n",
      "0.07032328\n",
      "0.06703433\n",
      "0.06387511\n",
      "0.060861535\n",
      "0.058010977\n",
      "0.05526499\n",
      "0.052666042\n",
      "0.0501882\n",
      "0.047824055\n",
      "0.04558172\n",
      "0.04343501\n",
      "0.04139182\n",
      "0.03944502\n",
      "0.037611164\n",
      "0.0358545\n",
      "0.03418664\n",
      "0.032568816\n",
      "0.031054426\n",
      "0.029598113\n",
      "0.028204944\n",
      "0.026894521\n",
      "0.025628705\n",
      "0.024438273\n",
      "0.023316616\n",
      "0.02222156\n",
      "0.021192232\n",
      "0.020200085\n",
      "0.01926241\n",
      "0.018378172\n",
      "0.017521087\n",
      "0.016714787\n",
      "0.01593959\n",
      "0.01519642\n",
      "0.014500254\n",
      "0.013831669\n",
      "0.013208991\n",
      "0.012592404\n",
      "0.012022189\n",
      "0.011469623\n",
      "0.010939813\n",
      "0.010446203\n",
      "0.009968607\n",
      "0.009511436\n",
      "0.009078478\n",
      "0.008670305\n",
      "0.008284921\n",
      "0.007904205\n",
      "0.0075423596\n",
      "0.0072071333\n",
      "0.0068862024\n",
      "0.006573025\n",
      "0.0062864423\n",
      "0.006007308\n",
      "0.005748598\n",
      "0.0054891896\n",
      "0.005239453\n",
      "0.0050163846\n",
      "0.0047936635\n",
      "0.004582222\n",
      "0.0043820213\n",
      "0.004193561\n",
      "0.004010462\n",
      "0.0038353698\n",
      "0.0036707956\n",
      "0.0035151348\n",
      "0.0033611828\n",
      "0.0032176257\n",
      "0.0030809143\n",
      "0.0029518167\n",
      "0.0028260346\n",
      "0.0027093336\n",
      "0.00259754\n",
      "0.0024881782\n",
      "0.0023867202\n",
      "0.002288671\n",
      "0.002193964\n",
      "0.002102846\n",
      "0.0020184845\n",
      "0.0019367359\n",
      "0.0018611663\n",
      "0.0017876562\n",
      "0.0017137385\n",
      "0.0016482632\n",
      "0.0015817875\n",
      "0.0015214648\n",
      "0.0014601253\n",
      "0.0014074221\n",
      "0.0013542816\n",
      "0.0013035468\n",
      "0.0012538559\n",
      "0.0012065005\n",
      "0.0011618724\n",
      "0.0011193334\n",
      "0.0010780118\n",
      "0.0010393081\n",
      "0.0010008159\n",
      "0.000964628\n",
      "0.0009321174\n",
      "0.00089958834\n",
      "0.0008680425\n",
      "0.00083813537\n",
      "0.0008094328\n",
      "0.0007826978\n",
      "0.0007549275\n",
      "0.00073018746\n",
      "0.0007047814\n",
      "0.00068003236\n",
      "0.00065715564\n",
      "0.0006363591\n",
      "0.0006169734\n",
      "0.00059744855\n",
      "0.0005783753\n",
      "0.00056038925\n",
      "0.00054377667\n",
      "0.0005263526\n",
      "0.0005103418\n",
      "0.0004946213\n",
      "0.00047953287\n",
      "0.00046582014\n",
      "0.0004518673\n",
      "0.00043790808\n",
      "0.00042541925\n",
      "0.00041225663\n",
      "0.00040108667\n",
      "0.0003893071\n",
      "0.00037960452\n",
      "0.00036857082\n",
      "0.00035840002\n",
      "0.00034712502\n",
      "0.0003382091\n",
      "0.00032783666\n",
      "0.00031941928\n",
      "0.00031025798\n",
      "0.00030198172\n",
      "0.0002943674\n",
      "0.00028647104\n",
      "0.00027783838\n",
      "0.0002714838\n",
      "0.00026534803\n",
      "0.000258859\n",
      "0.00025206056\n",
      "0.00024494843\n",
      "0.0002389196\n",
      "0.00023336006\n",
      "0.00022720538\n",
      "0.00022205559\n",
      "0.00021766254\n",
      "0.00021221138\n",
      "0.00020694638\n",
      "0.00020211644\n",
      "0.00019737748\n",
      "0.0001923613\n",
      "0.00018815052\n",
      "0.00018345228\n",
      "0.0001802267\n",
      "0.00017566214\n",
      "0.00017177983\n",
      "0.00016771401\n",
      "0.00016389458\n",
      "0.00016051279\n",
      "0.00015663236\n",
      "0.0001537324\n",
      "0.00015040585\n",
      "0.000147481\n",
      "0.0001445577\n",
      "0.00014132664\n",
      "0.00013809877\n",
      "0.00013504179\n",
      "0.0001318084\n",
      "0.00012956314\n",
      "0.00012691825\n",
      "0.00012414118\n",
      "0.00012156693\n",
      "0.00011982868\n",
      "0.000116566895\n",
      "0.000114553215\n",
      "0.00011248504\n",
      "0.0001104736\n",
      "0.000108295855\n",
      "0.00010624865\n",
      "0.000104063896\n",
      "0.000102029095\n",
      "0.00010062391\n",
      "9.86071e-05\n",
      "9.6930686e-05\n",
      "9.474523e-05\n",
      "9.345186e-05\n",
      "9.173577e-05\n",
      "8.99932e-05\n",
      "8.856677e-05\n",
      "8.717806e-05\n",
      "8.5748834e-05\n",
      "8.4573105e-05\n",
      "8.2839084e-05\n",
      "8.127758e-05\n",
      "7.998021e-05\n",
      "7.860303e-05\n",
      "7.731692e-05\n",
      "7.602158e-05\n",
      "7.466577e-05\n",
      "7.348634e-05\n",
      "7.267778e-05\n",
      "7.150133e-05\n",
      "7.041757e-05\n",
      "6.904483e-05\n",
      "6.785574e-05\n",
      "6.684903e-05\n",
      "6.60278e-05\n",
      "6.549322e-05\n",
      "6.425049e-05\n",
      "6.3542364e-05\n",
      "6.270649e-05\n",
      "6.1180304e-05\n",
      "6.059086e-05\n",
      "5.9681548e-05\n",
      "5.880308e-05\n",
      "5.7932804e-05\n",
      "5.7178084e-05\n",
      "5.616531e-05\n",
      "5.5537337e-05\n",
      "5.4959208e-05\n",
      "5.4200176e-05\n",
      "5.3302214e-05\n",
      "5.2756506e-05\n",
      "5.205848e-05\n",
      "5.1132756e-05\n",
      "5.0286013e-05\n",
      "4.9761187e-05\n",
      "4.9178972e-05\n",
      "4.8686146e-05\n",
      "4.820287e-05\n",
      "4.764429e-05\n",
      "4.6737903e-05\n",
      "4.6166235e-05\n",
      "4.5383757e-05\n",
      "4.4859902e-05\n",
      "4.4479653e-05\n",
      "4.4111664e-05\n",
      "4.318721e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%time\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
